{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMI 260 ASSIGNMENT #3 | Mammogram Spring 2018\n",
    "\n",
    "## Name 1:Joseph Nicolls\n",
    "\n",
    "## Name 2: Alex Lu\n",
    "\n",
    "Breast cancer has the highest incidence and second highest mortality rate for women in the US. \n",
    "Your task is to utilize machine learning to either classify AND/OR segment mammograms or neither, as long as you justify why it is useful to do whatever it is you want to do. If someone turns in a deep dream assignment using mammograms this might be amusing, but not so useful to patients. Consider this a mini-project, we highly suggest you work with 1 other person, it can be someone in your team. \n",
    "\n",
    "In addition to the mammograms, the dataset includes segmentations and mass_case_description_train_set.csv, which contains information about the mass shape, mass margins, assessment number, pathology diagnosis, and subtlety. Take some time to research what all of these different fields mean and whether you can use them in your algorithm or not. You dont need to use all of what is provided to you. \n",
    "\n",
    "Some ideas:\n",
    "\n",
    "1. use the ROIâ€™s or segmentations to extract features, and then train a classifier based on those features using the algorithms presented to you in the machine learning lectures, does not need to be deep learning. \n",
    "\n",
    "2. use convolutional neural networks, feel free to use any of the code we went over in class or use your own (custom code, sklearn, keras, Tensorflow etc.). If you dont want to place helper functions and classes into this notebook, place them in a .py file in the same folder called helperfunctions.py and import them into this notebook. \n",
    "\n",
    "The data is here:\n",
    "\n",
    "https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM\n",
    "\n",
    "If you do not like python, you can use a different language and turn your assignment in as a folder with all your code, a folder with all your figures and a latex or doc file with the writeup. The writeup doesnt need to be long, 1 page will do and cite at least one clinical paper and one technical paper. If you like python please place the writeup and code into this notebook. Use the markdown to tell is what you are doing in each section. You will not be graded on the performance of your model, only the scientific soundness of your claims, methodology, evaluation (ie fair but insightful statistics) and discussion of the shortcomings of what you tried. \n",
    "\n",
    "The format for this homework is as follows:\n",
    "\n",
    "1. Describe what you are doing and why it matters to patients using at least one citation\n",
    "\n",
    "2. Describe the relevant statistics of the data, how were the images taken ? how were they labeled ? what is the class balance and majority classifier accuracy ? How will you divide the data into testing, training and validation.\n",
    "\n",
    "3. Describe your data pipeline, ie how is the data scrubbed, normalized, stored and fed to the model for training. \n",
    "\n",
    "4. Explain how the model you chose works along side the code for it and at least one technical citation to give credit where credit is due. \n",
    "\n",
    "5. There are many ways to do training, take us through how you do it. (ie we used early stopping and we decided when to stop based on validation loss)  \n",
    "\n",
    "6. Make a figure displaying your results\n",
    "\n",
    "7. Discuss pros and cons of your method and what you might have done differently now that youve tried or would try if you had more time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional ML techniques for Classification of Mammograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Approach and Relevance\n",
    "\n",
    "Mammograms are difficult to classify because identifying features for malignant tumors can be vague and masses in mammograms can appear anywhere with any orientation in the breast tissue. Though the setup for this problem suggests use of CNNs, we want to use traditional ML techniques in this exploratory research for a variety of reasons. The principal reason that we want to do this is for the feature analysis possible with traditional ML technqiues that isn't possible with CNNs. According to a study from Britton et. all, sensitivity of radiologists in classifying mammograms can vary between 53.1-74.1%. Through feature analysis, indiciative features could be highlighted for radiologists to focus on, potentially raising sensitivites and decreasing the variability of senestivities between radiologists. In essence, we will attempt to classify the malginancy of tumors within mammograms using traditional machine learning techniques based on quanitative features derived from the mammogram and categorical features provided by expert analysis on those mammograms. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images were taken from the Digital Database for Screening Mammography (DDSM), a database which consists of 2620 mammogram studies. These images are X-rays, recorded as grey-scale images. Previous work had been done to filter these images to the ROI mass. Within features_matrix.csv, results of calculations on these masses have been recorded. We have didvided the data into training and testing at random, leaving 10% out for testing. 52% of these images are negatively labeled while 48% are positively labeled. \n",
    "\n",
    "In addition, semantic features associated with some on the images have also been included. These semantic features include the type of view included in the image, which side of the patient the breast was on, and case descriptions with a limited vocabulary describing characteristics of the mass. We believe that semantic features such as breast density, mass shape, and mass margins can be highly useful. The limited vocabulary of these qualitative features allows one-hot encoding and incorporation into our feature vectors for traditional ML methods. \n",
    "\n",
    "Relevant statistics in our data are based on the conventional wisdom that malignant tumors typically have irregular borders and a non spherical shape. In order to measure this, we compare the margins of the mass to \n",
    "its convex hull, or the smallest set of convex points which contains the ROI. Two statistics which provide a metric of this relationshiop is the convexity (the ratio of the convex hull perimeter to the ROI perimeter) \n",
    "and the solidity ( the ratio of the ROI area to the convex hull area.) The lower the convexity and lower the solidity, the less spherical the ROI is, potentially hinting at malginant character. \n",
    "\n",
    "In addition, we have also calculated a similar metric called extent. This metric is the ratio of the area of the ROI to the area of the bounding box of the ROI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try to leverage the following features in order to make attempts at classification: \n",
    "* features in the features_matrix.csv (ASM, Area, Centroid coordinates, ...) \n",
    "* convexity, solidity, and extent. (Calculated from masked ROI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the packages that we're going to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.morphology import convex_hull_image\n",
    "from skimage import data, img_as_float\n",
    "from skimage.util import invert\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.measure import regionprops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining global variables for paths, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_prop = .1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that calculates convex hull perimeter and area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_props(data): \n",
    "    ret,thresh_img = cv2.threshold(data,0,255,cv2.THRESH_BINARY)\n",
    "    thresh_props =  regionprops(thresh_img.astype(int))\n",
    "    thresh_area = thresh_props[0].area\n",
    "    thresh_perimeter = thresh_props[0].perimeter\n",
    "    chull = convex_hull_image(thresh_img)\n",
    "    props = regionprops(chull.astype(int))\n",
    "    chull_area = props[0].area\n",
    "    chull_perimeter = props[0].perimeter \n",
    "    convexity = chull_perimeter/thresh_perimeter\n",
    "    solidity = thresh_area/chull_area\n",
    "    return convexity, solidity, thresh_props[0].extent, thresh_props[0].major_axis_length, thresh_props[0].minor_axis_length\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that reads in image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_images(data_parent_dir):\n",
    "    '''\n",
    "        input:\n",
    "            * paths, a list of the paths that we need to input \n",
    "        output:\n",
    "            * a dataframe containing image data, label, and name \n",
    "    '''\n",
    "    print(\"preparing to read images\")\n",
    "    \n",
    "    # initialize data structures \n",
    "    data = []\n",
    "    label = []\n",
    "    name = []\n",
    "    convexities = []\n",
    "    solidities = []\n",
    "    extents = []\n",
    "    major_axis_lengths = []\n",
    "    minor_axis_lengths = []\n",
    "    \n",
    "    paths = os.listdir(data_parent_dir)\n",
    "\n",
    "    scan_ids = []\n",
    "    \n",
    "    # iteration over paths, dirs\n",
    "    for first_path in paths:\n",
    "        if first_path[0] == '.': # check for random dot files that come up :( \n",
    "            continue\n",
    "        local_dir = os.path.join(data_parent_dir, first_path)\n",
    "        for image in os.listdir(local_dir):\n",
    "            scan_ids.append(\"_\".join((first_path, image[:-3])))\n",
    "            with h5py.File(os.path.join(local_dir, image), 'r') as hf:\n",
    "                data.append(np.array(hf.get('data')))\n",
    "                label.append(np.array(hf.get('label')).item(0))\n",
    "                name.append(np.array(hf.get('name')))\n",
    "                \n",
    "                # compute additional features\n",
    "                convexity, solidity, extent, major_axis_length, minor_axis_length = get_props(hf.get('data')[:, :, 1])\n",
    "                convexities.append(convexity)\n",
    "                solidities.append(solidity)\n",
    "                extents.append(extent)\n",
    "                major_axis_lengths.append(major_axis_length)\n",
    "                minor_axis_lengths.append(minor_axis_length)\n",
    "         \n",
    "    print(scan_ids[:10])\n",
    "    scan_ids = [\"P_\" +scan_id for scan_id in scan_ids]\n",
    "    \n",
    "    d = {'pixel_data':data, 'label':label, 'name':name}\n",
    "    \n",
    "    d_computed = {'convexity': convexities, 'solidity': solidities, 'extent': extents, 'major_axis_length': major_axis_lengths, 'minor_axis_length': minor_axis_lengths}\n",
    "   \n",
    "    df_img = pd.DataFrame(data=d, index=scan_ids)\n",
    "    df_computed = pd.DataFrame(data=d_computed, index=scan_ids)\n",
    "\n",
    "    print(len(df[df['label'] == 0]))\n",
    "    print(len(df[df['label'] == 1]))\n",
    "    print(len(df))\n",
    "    \n",
    "    return (df_img, df_computed, scan_ids)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_excess_rows(scan_ids, precomputed_df):\n",
    "\n",
    "    drop_list = []\n",
    "    true_list = []\n",
    "    scan_ids = set(scan_ids)\n",
    "    names = precomputed_df.index.values\n",
    "    for name in names:\n",
    "        if name not in scan_ids:\n",
    "            drop_list.append(name)\n",
    "\n",
    "    print(\"the drop list has: \" + str(len(drop_list)))\n",
    "    return precomputed_df.drop(drop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially predict some potential issues with this approach: the number of features is large, and many are potentially correlated. For that reason, we can implement some dimensionality reduction and take a look at the variation explained by the principal components (and their contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(raw_data, new_dims=3):\n",
    "    '''\n",
    "        input:\n",
    "            * raw_data, the raw matrix that will be reduced in dimensionality \n",
    "        output:\n",
    "            * the dimensionality-reduced data \n",
    "        \n",
    "    '''\n",
    "    print(\"preparing to reduce dimensionality\")\n",
    "    pca = PCA()\n",
    "    pca.fit(raw_data)\n",
    "    print(\">>> variance explained by each principal component\")\n",
    "    print(pca.explained_variance_ratio_)  \n",
    "    print(\">>> the first principal component\")\n",
    "    print(pca.components_[0])\n",
    "    reduced = pca.transform(raw_data)[:,:new_dims]\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginnings of our preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_center_normalize(data):\n",
    "    data -= np.mean(data, axis = 0)\n",
    "    data /= np.std(data, axis = 0)\n",
    "    return data\n",
    "\n",
    "def preprocess(data_parent_dir, precomputed_path, dim_reduc=None):\n",
    "    print('Entering preprocessing')\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    # reading things in \n",
    "    image_df, computed_df, scan_ids = gather_images(data_parent_dir)\n",
    "    precomputed_df = drop_excess_rows(scan_ids, pd.read_csv(precomputed_path, index_col=0))\n",
    "\n",
    "    # an initial merge\n",
    "    feature_df = pd.merge(computed_df, precomputed_df, on=precomputed_df.index)\n",
    "    \n",
    "    # zero-centering, normalization of all data within the feature dataframe \n",
    "    features = feature_df.values[:,1:]\n",
    "    print(features.shape)\n",
    "    new_fts = mean_center_normalize(np.array(features,dtype=np.float32))\n",
    "    print(\"==============================\")\n",
    "    print(new_fts.shape)\n",
    "    \n",
    "    print(image_df.describe())\n",
    "    print(computed_df.describe())\n",
    "    print(precomputed_df.describe())\n",
    "    \n",
    "    # dimensionality reduction \n",
    "    if dim_reduc is not None:\n",
    "        new_fts = reduce_dimensionality(new_fts, dim_reduc)\n",
    "        feature_names += [\"pc# \"+str(x) for x in range(1, dim_reduc+1)]\n",
    "        feature_df = pd.DataFrame(new_fts, columns=feature_names, index=precomputed_df.index)\n",
    "        print(\"==============================\")\n",
    "        print(feature_df.info())\n",
    "        \n",
    "    # adding back to df \n",
    "    else:\n",
    "        feature_names += list(feature_df)[1:]\n",
    "        feature_df[feature_names] = new_fts\n",
    "        print(\"==============================\")\n",
    "        print(feature_df.info())\n",
    "    \n",
    "    print(feature_names)\n",
    "    \n",
    "    # joining dfs\n",
    "    df = pd.merge(image_df, feature_df[feature_names], on=feature_df.index)\n",
    "    df.info()\n",
    "    \n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=260)\n",
    "    \n",
    "    return (train[feature_names], train['label'], test[feature_names], test['label'], scan_ids)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've pretty much established how our data should be preprocessed, we want to introduce and train some basic machine learning models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train, labels):\n",
    "    \n",
    "    #lassoClf = Lasso()\n",
    "    #lassoClf.fit(train, labels)\n",
    "    \n",
    "    svcClf = SVC(random_state = 260)\n",
    "    svcClf.fit(train, labels)\n",
    "    \n",
    "    rfClf = RandomForestClassifier(random_state = 260)\n",
    "    rfClf.fit(train, labels)\n",
    "    \n",
    "    return [svcClf, rfClf]\n",
    "\n",
    "\n",
    "  #  return [lassoClf, svcClf, rfClf]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tuner(combined_features, groundtruth):\n",
    "    '''\n",
    "    Input: \n",
    "        * all features\n",
    "        * ground truth labels\n",
    "\n",
    "    Output:\n",
    "        * tuning grid of (accuracy, sensitivity, specificity) for specified bounds on C, class_weight\n",
    "\n",
    "    '''\n",
    "    C = .5\n",
    "    C_bound = 8\n",
    "    grid = np.zeros((C_bound, 3))\n",
    "    for i in range(C_bound):\n",
    "        grid[i] = EvaluateClassifier(combined_features, groundtruth, C)\n",
    "        C += .5\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateClassifier(X, Y, C):\n",
    "    '''\n",
    "    Input: \n",
    "        * Dataset X\n",
    "        * Labels Y\n",
    "    Output:\n",
    "        * outputs performance metrics: accuracy, sens, spec\n",
    "\n",
    "    Evaluates a linear-kernel support classifer with given penalty input C and melanoma class-weight weight\n",
    "    with 10-fold cross-validation. We must adjust the class weight to account for the class imbalance inherent \n",
    "    to the dataset. \n",
    "    '''\n",
    "\n",
    "    svm_man = SVC(C = C, kernel='linear')\n",
    "\n",
    "    scores = cross_val_score(svm_man, X, Y, cv=10, scoring='accuracy')\n",
    "    acc = scores.mean()\n",
    "    print('acc is:')\n",
    "    print(acc)\n",
    "\n",
    "    #------ Compute metrics ---------\n",
    "    y_predict = cross_val_predict(svm_man, X, Y, cv=10)\n",
    "    conf_mat = confusion_matrix(Y, y_predict)\n",
    "    tn, fp, fn, tp = conf_mat.ravel()\n",
    "    sens = float(tp)/(tp+fn)\n",
    "    spec = float(tn)/(tn+fp)\n",
    "    print('sens is:')\n",
    "    print(sens)\n",
    "    print('spec is:')\n",
    "    print(spec)\n",
    "    print(conf_mat)\n",
    "\n",
    "    return (acc, sens, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_models(models, test, y_true):\n",
    "    '''\n",
    "        input: \n",
    "            * models, a list of trained models \n",
    "            * test, the test set on which models will be evaluated\n",
    "            * feature_names, the list of feature columns that are being used \n",
    "    '''\n",
    "    print(\"Preparing to validate models\")\n",
    "    for model in models:\n",
    "        y_pred = model.predict(test)\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        #print(y_pred)\n",
    "        print(\"auroc: \" +  str(roc_auc_score(y_true, y_pred)))\n",
    "        \n",
    "    print(\"Done validating models\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parent_dir = \"./data_fixed_crop_w_mask\"\n",
    "precomputed_path = \"features_matrix.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering preprocessing\n",
      "preparing to read images\n",
      "['00001_LEFT_CC', '00001_LEFT_MLO', '00004_LEFT_CC', '00004_LEFT_MLO', '00004_RIGHT_CC', '00004_RIGHT_MLO', '00009_RIGHT_CC', '00009_RIGHT_MLO', '00015_LEFT_MLO', '00016_LEFT_CC']\n",
      "786\n",
      "722\n",
      "1508\n",
      "the drop list has: 91\n",
      "(1508, 23)\n",
      "==============================\n",
      "(1508, 23)\n",
      "             label\n",
      "count  1508.000000\n",
      "mean      0.478780\n",
      "std       0.499715\n",
      "min       0.000000\n",
      "25%       0.000000\n",
      "50%       0.000000\n",
      "75%       1.000000\n",
      "max       1.000000\n",
      "         convexity     solidity       extent  major_axis_length  \\\n",
      "count  1508.000000  1508.000000  1508.000000        1508.000000   \n",
      "mean      0.823225     0.863078     0.644197          90.195399   \n",
      "std       0.151674     0.117002     0.107757          62.893666   \n",
      "min       0.461828     0.047736     0.016178          31.827394   \n",
      "25%       0.754172     0.853156     0.620854          55.699932   \n",
      "50%       0.821002     0.885096     0.662134          68.380669   \n",
      "75%       0.876884     0.912385     0.699197          93.923356   \n",
      "max       3.014770     0.985606     0.873839         589.834634   \n",
      "\n",
      "       minor_axis_length  \n",
      "count        1508.000000  \n",
      "mean           70.213958  \n",
      "std            40.035041  \n",
      "min            23.460490  \n",
      "25%            45.367231  \n",
      "50%            57.932467  \n",
      "75%            77.418287  \n",
      "max           247.177242  \n",
      "               ASM           Area   Centroid_x   Centroid_y     Contrast  \\\n",
      "count  1508.000000    1508.000000  1508.000000  1508.000000  1508.000000   \n",
      "mean      0.989816   82196.687666     0.509939     0.516854     0.002132   \n",
      "std       0.009185   72112.671733     0.155333     0.245478     0.001284   \n",
      "min       0.905100    4567.000000     0.093025     0.019573     0.000200   \n",
      "25%       0.987500   38052.750000     0.392484     0.325622     0.001200   \n",
      "50%       0.992500   59975.000000     0.509351     0.522706     0.001900   \n",
      "75%       0.995700  100590.500000     0.624067     0.722545     0.002600   \n",
      "max       0.999200  656650.000000     0.923238     0.986207     0.009100   \n",
      "\n",
      "       Dissimilarity  Eccentricity       Energy  Homogeneity  Max intensity  \\\n",
      "count    1508.000000   1508.000000  1508.000000  1508.000000    1508.000000   \n",
      "mean        0.000978      0.569280     0.994883     0.999626     188.931134   \n",
      "std         0.000613      0.165532     0.004645     0.000258      31.375588   \n",
      "min         0.000100      0.149002     0.951300     0.997500      56.377432   \n",
      "25%         0.000600      0.459396     0.993700     0.999500     171.147860   \n",
      "50%         0.000800      0.570317     0.996300     0.999700     191.642023   \n",
      "75%         0.001200      0.671370     0.997800     0.999800     210.673617   \n",
      "max         0.005100      0.999369     0.999600     1.000000     255.000000   \n",
      "\n",
      "       Mean intensity  Min intensity  Orientation    Perimeter  \\\n",
      "count     1508.000000    1508.000000  1508.000000  1508.000000   \n",
      "mean       150.672146     102.798374     0.025581  1880.514226   \n",
      "std         32.871146      40.443288     0.890460   983.778491   \n",
      "min         29.622591       0.000000    -1.570040   370.143182   \n",
      "25%        131.409523      80.486381    -0.618421  1218.628859   \n",
      "50%        153.473748     108.038911     0.020111  1622.937300   \n",
      "75%        173.558266     131.247082     0.678797  2209.163743   \n",
      "max        231.298409     203.706897     1.570796  8591.265146   \n",
      "\n",
      "       Relative intensity     Solidity  StdDev intensity  Texture correlation  \n",
      "count         1508.000000  1508.000000       1508.000000          1508.000000  \n",
      "mean             0.740981     0.834554         10.139930             0.962781  \n",
      "std              0.123214     0.118032          5.354910             0.013902  \n",
      "min              0.221088     0.038955          0.982200             0.875400  \n",
      "25%              0.671168     0.817636          6.313000             0.955500  \n",
      "50%              0.751537     0.855264          9.042100             0.964800  \n",
      "75%              0.826171     0.886733         12.763475             0.972300  \n",
      "max              1.063823     0.989777         35.282200             0.994700  \n",
      "==============================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1508 entries, 0 to 1507\n",
      "Data columns (total 24 columns):\n",
      "key_0                  1508 non-null object\n",
      "convexity              1508 non-null float64\n",
      "solidity               1508 non-null float64\n",
      "extent                 1508 non-null float64\n",
      "major_axis_length      1508 non-null float64\n",
      "minor_axis_length      1508 non-null float64\n",
      "ASM                    1508 non-null float64\n",
      "Area                   1508 non-null float64\n",
      "Centroid_x             1508 non-null float64\n",
      "Centroid_y             1508 non-null float64\n",
      "Contrast               1508 non-null float64\n",
      "Dissimilarity          1508 non-null float64\n",
      "Eccentricity           1508 non-null float64\n",
      "Energy                 1508 non-null float64\n",
      "Homogeneity            1508 non-null float64\n",
      "Max intensity          1508 non-null float64\n",
      "Mean intensity         1508 non-null float64\n",
      "Min intensity          1508 non-null float64\n",
      "Orientation            1508 non-null float64\n",
      "Perimeter              1508 non-null float64\n",
      "Relative intensity     1508 non-null float64\n",
      "Solidity               1508 non-null float64\n",
      "StdDev intensity       1508 non-null float64\n",
      "Texture correlation    1508 non-null float64\n",
      "dtypes: float64(23), object(1)\n",
      "memory usage: 294.5+ KB\n",
      "None\n",
      "['convexity', 'solidity', 'extent', 'major_axis_length', 'minor_axis_length', 'ASM', 'Area', 'Centroid_x', 'Centroid_y', 'Contrast', 'Dissimilarity', 'Eccentricity', 'Energy', 'Homogeneity', 'Max intensity', 'Mean intensity', 'Min intensity', 'Orientation', 'Perimeter', 'Relative intensity', 'Solidity', 'StdDev intensity', 'Texture correlation']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1508 entries, 0 to 1507\n",
      "Data columns (total 27 columns):\n",
      "key_0                  1508 non-null int64\n",
      "pixel_data             1508 non-null object\n",
      "label                  1508 non-null int64\n",
      "name                   1508 non-null object\n",
      "convexity              1508 non-null float64\n",
      "solidity               1508 non-null float64\n",
      "extent                 1508 non-null float64\n",
      "major_axis_length      1508 non-null float64\n",
      "minor_axis_length      1508 non-null float64\n",
      "ASM                    1508 non-null float64\n",
      "Area                   1508 non-null float64\n",
      "Centroid_x             1508 non-null float64\n",
      "Centroid_y             1508 non-null float64\n",
      "Contrast               1508 non-null float64\n",
      "Dissimilarity          1508 non-null float64\n",
      "Eccentricity           1508 non-null float64\n",
      "Energy                 1508 non-null float64\n",
      "Homogeneity            1508 non-null float64\n",
      "Max intensity          1508 non-null float64\n",
      "Mean intensity         1508 non-null float64\n",
      "Min intensity          1508 non-null float64\n",
      "Orientation            1508 non-null float64\n",
      "Perimeter              1508 non-null float64\n",
      "Relative intensity     1508 non-null float64\n",
      "Solidity               1508 non-null float64\n",
      "StdDev intensity       1508 non-null float64\n",
      "Texture correlation    1508 non-null float64\n",
      "dtypes: float64(23), int64(2), object(2)\n",
      "memory usage: 329.9+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, all_IDs = preprocess(data_parent_dir, precomputed_path, dim_reduc=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to validate models\n",
      "[[102  51]\n",
      " [ 78  71]]\n",
      "auroc: 0.5715883668903804\n",
      "[[100  53]\n",
      " [ 92  57]]\n",
      "auroc: 0.5180725534061499\n",
      "Done validating models\n"
     ]
    }
   ],
   "source": [
    "models = train_models(X_train, y_train)\n",
    "validate_models(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now that we've seen our models perform like garbage, we're going to try to add in the semantic feature encoding to see if that will help "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_semantic_features(semantics_path):\n",
    "    semantic_df = pd.read_csv(semantics_path)\n",
    "    semantic_df.dropna(inplace=True)\n",
    "    return semantic_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_labels(semantic_df, semantic_feature_names):\n",
    "    '''\n",
    "        input:\n",
    "        \n",
    "        output:\n",
    "        \n",
    "    '''\n",
    "    for feature in semantic_feature_names:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(list(semantic_df[feature].astype(str)))\n",
    "        semantic_df[feature] = le.transform(semantic_df[feature])\n",
    "    return semantic_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(semantic_df, semantic_feature_names):\n",
    "    '''\n",
    "        input:\n",
    "        \n",
    "        output:\n",
    "        \n",
    "    '''\n",
    "    enc = preprocessing.OneHotEncoder(sparse=False)\n",
    "    semantic_one_hots = enc.fit_transform(semantic_df[semantic_feature_names])\n",
    "    _, one_hot_length = semantic_one_hots.shape\n",
    "    return (semantic_one_hots, one_hot_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_semantic_df(semantics_path, semantic_feature_names, total_patientIDs):\n",
    "    \n",
    "    semantic_df = gather_semantic_features(semantics_path)\n",
    "    semantic_df = encode_categorical_labels(semantic_df, semantic_feature_names)\n",
    "    semantic_one_hots, one_hot_length = one_hot_encoding(semantic_df, semantic_feature_names)\n",
    "   \n",
    "    has_semantic = [s1 + \"_\" + s2 + \"_\" + s3 for (s1, s2, s3) in zip(list(semantic_df['patient_id']), list(semantic_df['side']), list(semantic_df['view']))]\n",
    "\n",
    "    semantic_encoded_dict = {img:np.zeros(one_hot_length) for (idx, img) in enumerate(total_patientIDs)}\n",
    "\n",
    "    for img, patient_id in enumerate(has_semantic): \n",
    "        if patient_id in semantic_encoded_dict.keys():\n",
    "            semantic_encoded_dict[patient_id] = semantic_one_hots[img] \n",
    "        \n",
    "    print(one_hot_length)\n",
    "    print(len(total_patientIDs))\n",
    "    \n",
    "    encoded_feature_names = [\"one_hot #\" + str(x) for x in range(1, one_hot_length+1)]\n",
    "    semantic_encoded_df = pd.DataFrame.from_dict(semantic_encoded_dict, orient='index', columns=encoded_feature_names)\n",
    "    return (semantic_encoded_df, encoded_feature_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we find that there are semantic descriptions of images that do not appear in the h5'd dataset, and also images that appear in this dataset without corresponding semantic descriptions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(data_parent_dir, precomputed_path, semantics_path, semantic_feature_names, dim_reduc=None):\n",
    "    print('Entering preprocessing')\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    # reading things in \n",
    "    image_df, computed_df, scan_ids = gather_images(data_parent_dir)\n",
    "    precomputed_df = drop_excess_rows(scan_ids, pd.read_csv(precomputed_path, index_col=0))\n",
    "\n",
    "    # an initial merge\n",
    "    feature_df = pd.merge(computed_df, precomputed_df, on=precomputed_df.index)\n",
    "    \n",
    "    # zero-centering, normalization of all data within the feature dataframe \n",
    "    features = feature_df.values[:,1:]\n",
    "    new_fts = mean_center_normalize(np.array(features,dtype=np.float32))\n",
    "\n",
    "    print(image_df.describe())\n",
    "    print(computed_df.describe())\n",
    "    print(precomputed_df.describe())\n",
    "    \n",
    "    # dimensionality reduction \n",
    "    if dim_reduc is not None:\n",
    "        new_fts = reduce_dimensionality(new_fts, dim_reduc)\n",
    "        feature_names += [\"pc# \"+str(x) for x in range(1, dim_reduc+1)]\n",
    "        feature_df = pd.DataFrame(new_fts, columns=feature_names, index=precomputed_df.index)\n",
    "\n",
    "\n",
    "        \n",
    "    # adding back to df \n",
    "    else:\n",
    "        feature_names += list(feature_df)[1:]\n",
    "        feature_df[feature_names] = new_fts\n",
    "    \n",
    "    # get the semantic df\n",
    "    semantic_df, encoded_feature_names = generate_semantic_df(semantics_path, semantic_feature_names, scan_ids)\n",
    "    \n",
    "    feature_names += encoded_feature_names\n",
    "    \n",
    "    # joining dfs\n",
    "    df = pd.merge(image_df, feature_df[feature_names], on=feature_df.index)\n",
    "    df = pd.merge(df, semantic_df, on=df.index)\n",
    "    df.info()\n",
    "    \n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=260)\n",
    "    \n",
    "    return (train[feature_names], train['label'], test[feature_names], test['label'], scan_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_label_name = ['pathology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing to read images\n"
     ]
    }
   ],
   "source": [
    "image_df, computed_df, scan_ids = gather_images(data_parent_dir)\n",
    "semantic_feature_names = ['breast_density', 'abn_num', 'mass_shape', 'mass_margins', 'assessment']\n",
    "semantics_path = \"mass_case_description_train_set.csv\"\n",
    "\n",
    "\n",
    "semantic_df, encoded_feature_names = generate_semantic_df(semantics_path, semantic_feature_names, scan_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['P_00001_LEFT_CC', 'P_00001_LEFT_MLO', 'P_00004_LEFT_CC',\n",
      "       'P_00004_LEFT_MLO', 'P_00004_RIGHT_CC', 'P_00004_RIGHT_MLO',\n",
      "       'P_00009_RIGHT_CC', 'P_00009_RIGHT_MLO', 'P_00015_LEFT_MLO',\n",
      "       'P_00016_LEFT_CC',\n",
      "       ...\n",
      "       'P_01550_RIGHT_CC', 'P_01550_RIGHT_MLO', 'P_01551_LEFT_CC',\n",
      "       'P_01551_LEFT_MLO', 'P_01553_RIGHT_CC', 'P_01553_RIGHT_MLO',\n",
      "       'P_01555_LEFT_MLO', 'P_01556_LEFT_CC', 'P_01557_RIGHT_CC',\n",
      "       'P_01557_RIGHT_MLO'],\n",
      "      dtype='object', length=1508)\n"
     ]
    }
   ],
   "source": [
    "print(semantic_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering preprocessing\n",
      "preparing to read images\n",
      "['00001_LEFT_CC', '00001_LEFT_MLO', '00004_LEFT_CC', '00004_LEFT_MLO', '00004_RIGHT_CC', '00004_RIGHT_MLO', '00009_RIGHT_CC', '00009_RIGHT_MLO', '00015_LEFT_MLO', '00016_LEFT_CC']\n",
      "786\n",
      "722\n",
      "1508\n",
      "the drop list has: 91\n",
      "(1508, 23)\n",
      "==============================\n",
      "(1508, 23)\n",
      "             label\n",
      "count  1508.000000\n",
      "mean      0.478780\n",
      "std       0.499715\n",
      "min       0.000000\n",
      "25%       0.000000\n",
      "50%       0.000000\n",
      "75%       1.000000\n",
      "max       1.000000\n",
      "         convexity     solidity       extent  major_axis_length  \\\n",
      "count  1508.000000  1508.000000  1508.000000        1508.000000   \n",
      "mean      0.823225     0.863078     0.644197          90.195399   \n",
      "std       0.151674     0.117002     0.107757          62.893666   \n",
      "min       0.461828     0.047736     0.016178          31.827394   \n",
      "25%       0.754172     0.853156     0.620854          55.699932   \n",
      "50%       0.821002     0.885096     0.662134          68.380669   \n",
      "75%       0.876884     0.912385     0.699197          93.923356   \n",
      "max       3.014770     0.985606     0.873839         589.834634   \n",
      "\n",
      "       minor_axis_length  \n",
      "count        1508.000000  \n",
      "mean           70.213958  \n",
      "std            40.035041  \n",
      "min            23.460490  \n",
      "25%            45.367231  \n",
      "50%            57.932467  \n",
      "75%            77.418287  \n",
      "max           247.177242  \n",
      "               ASM           Area   Centroid_x   Centroid_y     Contrast  \\\n",
      "count  1508.000000    1508.000000  1508.000000  1508.000000  1508.000000   \n",
      "mean      0.989816   82196.687666     0.509939     0.516854     0.002132   \n",
      "std       0.009185   72112.671733     0.155333     0.245478     0.001284   \n",
      "min       0.905100    4567.000000     0.093025     0.019573     0.000200   \n",
      "25%       0.987500   38052.750000     0.392484     0.325622     0.001200   \n",
      "50%       0.992500   59975.000000     0.509351     0.522706     0.001900   \n",
      "75%       0.995700  100590.500000     0.624067     0.722545     0.002600   \n",
      "max       0.999200  656650.000000     0.923238     0.986207     0.009100   \n",
      "\n",
      "       Dissimilarity  Eccentricity       Energy  Homogeneity  Max intensity  \\\n",
      "count    1508.000000   1508.000000  1508.000000  1508.000000    1508.000000   \n",
      "mean        0.000978      0.569280     0.994883     0.999626     188.931134   \n",
      "std         0.000613      0.165532     0.004645     0.000258      31.375588   \n",
      "min         0.000100      0.149002     0.951300     0.997500      56.377432   \n",
      "25%         0.000600      0.459396     0.993700     0.999500     171.147860   \n",
      "50%         0.000800      0.570317     0.996300     0.999700     191.642023   \n",
      "75%         0.001200      0.671370     0.997800     0.999800     210.673617   \n",
      "max         0.005100      0.999369     0.999600     1.000000     255.000000   \n",
      "\n",
      "       Mean intensity  Min intensity  Orientation    Perimeter  \\\n",
      "count     1508.000000    1508.000000  1508.000000  1508.000000   \n",
      "mean       150.672146     102.798374     0.025581  1880.514226   \n",
      "std         32.871146      40.443288     0.890460   983.778491   \n",
      "min         29.622591       0.000000    -1.570040   370.143182   \n",
      "25%        131.409523      80.486381    -0.618421  1218.628859   \n",
      "50%        153.473748     108.038911     0.020111  1622.937300   \n",
      "75%        173.558266     131.247082     0.678797  2209.163743   \n",
      "max        231.298409     203.706897     1.570796  8591.265146   \n",
      "\n",
      "       Relative intensity     Solidity  StdDev intensity  Texture correlation  \n",
      "count         1508.000000  1508.000000       1508.000000          1508.000000  \n",
      "mean             0.740981     0.834554         10.139930             0.962781  \n",
      "std              0.123214     0.118032          5.354910             0.013902  \n",
      "min              0.221088     0.038955          0.982200             0.875400  \n",
      "25%              0.671168     0.817636          6.313000             0.955500  \n",
      "50%              0.751537     0.855264          9.042100             0.964800  \n",
      "75%              0.826171     0.886733         12.763475             0.972300  \n",
      "max              1.063823     0.989777         35.282200             0.994700  \n",
      "preparing to reduce dimensionality\n",
      ">>> variance explained by each principal component\n",
      "[3.76943260e-01 1.76522940e-01 1.19853616e-01 5.62588200e-02\n",
      " 4.47084233e-02 4.44010571e-02 4.13592122e-02 3.59365381e-02\n",
      " 2.85934005e-02 2.35507470e-02 1.67529359e-02 1.42034749e-02\n",
      " 6.66213036e-03 5.10428147e-03 3.13439104e-03 2.34590913e-03\n",
      " 1.80512597e-03 8.77200509e-04 6.77150034e-04 2.12909246e-04\n",
      " 8.96651982e-05 5.40107294e-06 1.41128305e-06]\n",
      ">>> the first principal component\n",
      "[-0.05010179 -0.11893838 -0.10367503  0.18981083  0.17931384 -0.31497788\n",
      "  0.29259795 -0.04996694 -0.01063207  0.29834715  0.3081699   0.03477044\n",
      " -0.31463924 -0.29181573  0.20738153  0.15343374  0.01110461  0.00149909\n",
      "  0.29819408  0.20740704 -0.09723203  0.31694236  0.19778115]\n",
      "==============================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1508 entries, P_00001_LEFT_CC to P_01557_RIGHT_MLO\n",
      "Data columns (total 3 columns):\n",
      "pc# 1    1508 non-null float32\n",
      "pc# 2    1508 non-null float32\n",
      "pc# 3    1508 non-null float32\n",
      "dtypes: float32(3)\n",
      "memory usage: 29.5+ KB\n",
      "None\n",
      "['1' '2' '3' '4']\n",
      "['1' '2' '3' '4' '5' '6']\n",
      "['ARCHITECTURAL_DISTORTION' 'ASYMMETRIC_BREAST_TISSUE'\n",
      " 'FOCAL_ASYMMETRIC_DENSITY' 'IRREGULAR'\n",
      " 'IRREGULAR-ARCHITECTURAL_DISTORTION' 'IRREGULAR-FOCAL_ASYMMETRIC_DENSITY'\n",
      " 'LOBULATED' 'LOBULATED-ARCHITECTURAL_DISTORTION' 'LOBULATED-IRREGULAR'\n",
      " 'LOBULATED-LYMPH_NODE' 'LOBULATED-OVAL' 'LYMPH_NODE' 'OVAL'\n",
      " 'OVAL-LYMPH_NODE' 'ROUND' 'ROUND-IRREGULAR-ARCHITECTURAL_DISTORTION'\n",
      " 'ROUND-LOBULATED' 'ROUND-OVAL']\n",
      "['CIRCUMSCRIBED' 'CIRCUMSCRIBED-ILL_DEFINED'\n",
      " 'CIRCUMSCRIBED-MICROLOBULATED' 'CIRCUMSCRIBED-OBSCURED' 'ILL_DEFINED'\n",
      " 'ILL_DEFINED-SPICULATED' 'MICROLOBULATED' 'MICROLOBULATED-ILL_DEFINED'\n",
      " 'MICROLOBULATED-ILL_DEFINED-SPICULATED' 'MICROLOBULATED-SPICULATED'\n",
      " 'OBSCURED' 'OBSCURED-ILL_DEFINED' 'OBSCURED-ILL_DEFINED-SPICULATED'\n",
      " 'OBSCURED-SPICULATED' 'SPICULATED']\n",
      "['0' '1' '2' '3' '4' '5']\n",
      "[[0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]]\n",
      "49\n",
      "332\n",
      "1508\n",
      "        one_hot #1   one_hot #2   one_hot #3   one_hot #4   one_hot #5  \\\n",
      "count  1508.000000  1508.000000  1508.000000  1508.000000  1508.000000   \n",
      "mean      0.107427     0.253316     0.165119     0.055040     0.547082   \n",
      "std       0.309758     0.435054     0.371411     0.228134     0.497943   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
      "75%       0.000000     1.000000     0.000000     0.000000     1.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "        one_hot #6   one_hot #7   one_hot #8   one_hot #9  one_hot #10  \\\n",
      "count  1508.000000  1508.000000  1508.000000  1508.000000  1508.000000   \n",
      "mean      0.021220     0.008621     0.001989     0.000663     0.001326   \n",
      "std       0.144165     0.092477     0.044573     0.025751     0.036406   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "          ...       one_hot #40  one_hot #41  one_hot #42  one_hot #43  \\\n",
      "count     ...       1508.000000  1508.000000  1508.000000  1508.000000   \n",
      "mean      ...          0.005968     0.002653     0.002653     0.138594   \n",
      "std       ...          0.077049     0.051451     0.051451     0.345637   \n",
      "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "75%       ...          0.000000     0.000000     0.000000     0.000000   \n",
      "max       ...          1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "       one_hot #44  one_hot #45  one_hot #46  one_hot #47  one_hot #48  \\\n",
      "count  1508.000000  1508.000000  1508.000000  1508.000000  1508.000000   \n",
      "mean      0.047745     0.000663     0.015915     0.111406     0.253979   \n",
      "std       0.213298     0.025751     0.125189     0.314738     0.435430   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "75%       0.000000     0.000000     0.000000     0.000000     1.000000   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "       one_hot #49  \n",
      "count  1508.000000  \n",
      "mean      0.151194  \n",
      "std       0.358356  \n",
      "min       0.000000  \n",
      "25%       0.000000  \n",
      "50%       0.000000  \n",
      "75%       0.000000  \n",
      "max       1.000000  \n",
      "\n",
      "[8 rows x 49 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['one_hot #1' 'one_hot #2' 'one_hot #3' 'one_hot #4' 'one_hot #5'\\n 'one_hot #6' 'one_hot #7' 'one_hot #8' 'one_hot #9' 'one_hot #10'\\n 'one_hot #11' 'one_hot #12' 'one_hot #13' 'one_hot #14' 'one_hot #15'\\n 'one_hot #16' 'one_hot #17' 'one_hot #18' 'one_hot #19' 'one_hot #20'\\n 'one_hot #21' 'one_hot #22' 'one_hot #23' 'one_hot #24' 'one_hot #25'\\n 'one_hot #26' 'one_hot #27' 'one_hot #28' 'one_hot #29' 'one_hot #30'\\n 'one_hot #31' 'one_hot #32' 'one_hot #33' 'one_hot #34' 'one_hot #35'\\n 'one_hot #36' 'one_hot #37' 'one_hot #38' 'one_hot #39' 'one_hot #40'\\n 'one_hot #41' 'one_hot #42' 'one_hot #43' 'one_hot #44' 'one_hot #45'\\n 'one_hot #46' 'one_hot #47' 'one_hot #48' 'one_hot #49'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-7208af69dbb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                        \u001b[0msemantics_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                        \u001b[0msemantic_feature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                                                        dim_reduc=3)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-153-7d680fcdb525>\u001b[0m in \u001b[0;36mpreprocess2\u001b[0;34m(data_parent_dir, precomputed_path, semantics_path, semantic_feature_names, dim_reduc)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# joining dfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemantic_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2677\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2678\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2679\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2680\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2722\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2723\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2724\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[0;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[1;32m   1325\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[0;32m-> 1327\u001b[0;31m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['one_hot #1' 'one_hot #2' 'one_hot #3' 'one_hot #4' 'one_hot #5'\\n 'one_hot #6' 'one_hot #7' 'one_hot #8' 'one_hot #9' 'one_hot #10'\\n 'one_hot #11' 'one_hot #12' 'one_hot #13' 'one_hot #14' 'one_hot #15'\\n 'one_hot #16' 'one_hot #17' 'one_hot #18' 'one_hot #19' 'one_hot #20'\\n 'one_hot #21' 'one_hot #22' 'one_hot #23' 'one_hot #24' 'one_hot #25'\\n 'one_hot #26' 'one_hot #27' 'one_hot #28' 'one_hot #29' 'one_hot #30'\\n 'one_hot #31' 'one_hot #32' 'one_hot #33' 'one_hot #34' 'one_hot #35'\\n 'one_hot #36' 'one_hot #37' 'one_hot #38' 'one_hot #39' 'one_hot #40'\\n 'one_hot #41' 'one_hot #42' 'one_hot #43' 'one_hot #44' 'one_hot #45'\\n 'one_hot #46' 'one_hot #47' 'one_hot #48' 'one_hot #49'] not in index\""
     ]
    }
   ],
   "source": [
    "data_parent_dir = \"./data_fixed_crop_w_mask\"\n",
    "precomputed_path = \"features_matrix.csv\"\n",
    "semantics_path = \"mass_case_description_train_set.csv\"\n",
    "\n",
    "semantic_feature_names = ['breast_density', 'abn_num', 'mass_shape', 'mass_margins', 'assessment']\n",
    "\n",
    "X_train, y_train, X_test, y_test = preprocess2(data_parent_dir, \n",
    "                                                       precomputed_path, \n",
    "                                                       semantics_path, \n",
    "                                                       semantic_feature_names, \n",
    "                                                       dim_reduc=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering preprocessing\n",
      "preparing to read images\n",
      "786\n",
      "722\n",
      "1508\n",
      "preparing to reduce dimensionality\n",
      ">>> variance explained by each principal component\n",
      "[4.4473657e-01 1.6824159e-01 9.1169290e-02 5.7367690e-02 5.6748547e-02\n",
      " 5.2320685e-02 3.6198188e-02 3.1822890e-02 2.2656877e-02 1.7601196e-02\n",
      " 9.3034180e-03 7.1804966e-03 2.3674807e-03 1.1376145e-03 8.7159796e-04\n",
      " 2.6753976e-04 6.5508975e-06 1.8152512e-06]\n",
      ">>> the first principal component\n",
      "[-0.33582258  0.3129383  -0.04206447 -0.01204051  0.31197217  0.3208049\n",
      "  0.00518964 -0.33550972 -0.30273733  0.21556541  0.16609216  0.02282244\n",
      "  0.00318173  0.30442756  0.21560022 -0.05786494  0.3394732   0.22484674]\n",
      "['1' '2' '3' '4']\n",
      "['1' '2' '3' '4' '5' '6']\n",
      "['ARCHITECTURAL_DISTORTION' 'ASYMMETRIC_BREAST_TISSUE'\n",
      " 'FOCAL_ASYMMETRIC_DENSITY' 'IRREGULAR'\n",
      " 'IRREGULAR-ARCHITECTURAL_DISTORTION' 'IRREGULAR-FOCAL_ASYMMETRIC_DENSITY'\n",
      " 'LOBULATED' 'LOBULATED-ARCHITECTURAL_DISTORTION' 'LOBULATED-IRREGULAR'\n",
      " 'LOBULATED-LYMPH_NODE' 'LOBULATED-OVAL' 'LYMPH_NODE' 'OVAL'\n",
      " 'OVAL-LYMPH_NODE' 'ROUND' 'ROUND-IRREGULAR-ARCHITECTURAL_DISTORTION'\n",
      " 'ROUND-LOBULATED' 'ROUND-OVAL']\n",
      "['CIRCUMSCRIBED' 'CIRCUMSCRIBED-ILL_DEFINED'\n",
      " 'CIRCUMSCRIBED-MICROLOBULATED' 'CIRCUMSCRIBED-OBSCURED' 'ILL_DEFINED'\n",
      " 'ILL_DEFINED-SPICULATED' 'MICROLOBULATED' 'MICROLOBULATED-ILL_DEFINED'\n",
      " 'MICROLOBULATED-ILL_DEFINED-SPICULATED' 'MICROLOBULATED-SPICULATED'\n",
      " 'OBSCURED' 'OBSCURED-ILL_DEFINED' 'OBSCURED-ILL_DEFINED-SPICULATED'\n",
      " 'OBSCURED-SPICULATED' 'SPICULATED']\n",
      "['0' '1' '2' '3' '4' '5']\n",
      "[[0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]]\n",
      "49\n",
      "1273\n",
      "1599\n",
      "       one_hot #1  one_hot #2  one_hot #3  one_hot #4  one_hot #5  one_hot #6  \\\n",
      "count      1599.0      1599.0      1599.0      1599.0      1599.0      1599.0   \n",
      "mean          0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "std           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "min           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "25%           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "50%           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "75%           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "max           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "       one_hot #7  one_hot #8  one_hot #9  one_hot #10     ...       \\\n",
      "count      1599.0      1599.0      1599.0       1599.0     ...        \n",
      "mean          0.0         0.0         0.0          0.0     ...        \n",
      "std           0.0         0.0         0.0          0.0     ...        \n",
      "min           0.0         0.0         0.0          0.0     ...        \n",
      "25%           0.0         0.0         0.0          0.0     ...        \n",
      "50%           0.0         0.0         0.0          0.0     ...        \n",
      "75%           0.0         0.0         0.0          0.0     ...        \n",
      "max           0.0         0.0         0.0          0.0     ...        \n",
      "\n",
      "       one_hot #40  one_hot #41  one_hot #42  one_hot #43  one_hot #44  \\\n",
      "count       1599.0       1599.0       1599.0       1599.0       1599.0   \n",
      "mean           0.0          0.0          0.0          0.0          0.0   \n",
      "std            0.0          0.0          0.0          0.0          0.0   \n",
      "min            0.0          0.0          0.0          0.0          0.0   \n",
      "25%            0.0          0.0          0.0          0.0          0.0   \n",
      "50%            0.0          0.0          0.0          0.0          0.0   \n",
      "75%            0.0          0.0          0.0          0.0          0.0   \n",
      "max            0.0          0.0          0.0          0.0          0.0   \n",
      "\n",
      "       one_hot #45  one_hot #46  one_hot #47  one_hot #48  one_hot #49  \n",
      "count       1599.0       1599.0       1599.0       1599.0       1599.0  \n",
      "mean           0.0          0.0          0.0          0.0          0.0  \n",
      "std            0.0          0.0          0.0          0.0          0.0  \n",
      "min            0.0          0.0          0.0          0.0          0.0  \n",
      "25%            0.0          0.0          0.0          0.0          0.0  \n",
      "50%            0.0          0.0          0.0          0.0          0.0  \n",
      "75%            0.0          0.0          0.0          0.0          0.0  \n",
      "max            0.0          0.0          0.0          0.0          0.0  \n",
      "\n",
      "[8 rows x 49 columns]\n",
      "             label        pc# 1        pc# 2        pc# 3  one_hot #1  \\\n",
      "count  1508.000000  1508.000000  1508.000000  1508.000000         0.0   \n",
      "mean      0.478780    -0.020880    -0.003538     0.006279         NaN   \n",
      "std       0.499715     2.824638     1.738863     1.296816         NaN   \n",
      "min       0.000000    -5.887184    -4.303214    -3.578173         NaN   \n",
      "25%       0.000000    -1.916224    -1.176806    -0.759557         NaN   \n",
      "50%       0.000000    -0.533474    -0.170123    -0.128470         NaN   \n",
      "75%       1.000000     1.197309     0.971187     0.506527         NaN   \n",
      "max       1.000000    18.382227     5.955268     7.663785         NaN   \n",
      "\n",
      "       one_hot #2  one_hot #3  one_hot #4  one_hot #5  one_hot #6  \\\n",
      "count         0.0         0.0         0.0         0.0         0.0   \n",
      "mean          NaN         NaN         NaN         NaN         NaN   \n",
      "std           NaN         NaN         NaN         NaN         NaN   \n",
      "min           NaN         NaN         NaN         NaN         NaN   \n",
      "25%           NaN         NaN         NaN         NaN         NaN   \n",
      "50%           NaN         NaN         NaN         NaN         NaN   \n",
      "75%           NaN         NaN         NaN         NaN         NaN   \n",
      "max           NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "          ...       one_hot #40  one_hot #41  one_hot #42  one_hot #43  \\\n",
      "count     ...               0.0          0.0          0.0          0.0   \n",
      "mean      ...               NaN          NaN          NaN          NaN   \n",
      "std       ...               NaN          NaN          NaN          NaN   \n",
      "min       ...               NaN          NaN          NaN          NaN   \n",
      "25%       ...               NaN          NaN          NaN          NaN   \n",
      "50%       ...               NaN          NaN          NaN          NaN   \n",
      "75%       ...               NaN          NaN          NaN          NaN   \n",
      "max       ...               NaN          NaN          NaN          NaN   \n",
      "\n",
      "       one_hot #44  one_hot #45  one_hot #46  one_hot #47  one_hot #48  \\\n",
      "count          0.0          0.0          0.0          0.0          0.0   \n",
      "mean           NaN          NaN          NaN          NaN          NaN   \n",
      "std            NaN          NaN          NaN          NaN          NaN   \n",
      "min            NaN          NaN          NaN          NaN          NaN   \n",
      "25%            NaN          NaN          NaN          NaN          NaN   \n",
      "50%            NaN          NaN          NaN          NaN          NaN   \n",
      "75%            NaN          NaN          NaN          NaN          NaN   \n",
      "max            NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "       one_hot #49  \n",
      "count          0.0  \n",
      "mean           NaN  \n",
      "std            NaN  \n",
      "min            NaN  \n",
      "25%            NaN  \n",
      "50%            NaN  \n",
      "75%            NaN  \n",
      "max            NaN  \n",
      "\n",
      "[8 rows x 53 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-53afb9f04d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-73745680611b>\u001b[0m in \u001b[0;36mpipeline2\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                            \u001b[0msemantic_feature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                            dim_reduc=3)\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalidate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-3db7ad794d61>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(train, labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msvcClf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m260\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msvcClf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrfClf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m260\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "models = train_models(X_train, y_train)\n",
    "validate_models(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "As mentioned in our introduction, vanilla ML is significantly more interpretable and parsable in terms of the importance of specific variables. In addition, vanilla ML allows us to incoporate expert opinion\n",
    "and analysis into our models. Because of this, our models do not need to learn specific relationships between masses and malignant character that is already common domain knowledge; most features focus on the \n",
    "irregularity of the mass's borders and deviance from sphericality. \n",
    "\n",
    "\n",
    "For future work, more feature analysis could potentially result in useful findings and recommendations for pathologists who are examining mammograms. In addition, it would be worthwhile to compare the\n",
    "performance on this model to a CNN to see which images can be misclassified due to dependence on conventional metrics. Finding examples of such images that are successfully classified under CNN but not \n",
    "not correctly classified under vanilla ML approaches would provide excellent examples to provide to pathlogists to warn them about the unconventional nature of different types of masses. In addition, due to both \n",
    "lack of time and the limited scope of the data provided, we were unable to to train models based on different orientations of mages. Training different models for different orientions could provide useful\n",
    "feedback on what variables are more indicative of a mass's malignant character in different kinds of images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Bradski, G. (2000). The OpenCV Library. Dr. Dobb's Journal of Software Tools\n",
    "\n",
    "Britton P, Warwick J, Wallis MG, et al. Measuring the accuracy of diagnostic imaging in symptomatic breast patients: team and individual performance. The British Journal of Radiology. 2012;85(1012):415-422. doi:10.1259/bjr/32906819.\n",
    "\n",
    "Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "\n",
    "StÃ©fan van der Walt, Johannes L. SchÃ¶nberger, Juan Nunez-Iglesias, FranÃ§ois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu and the scikit-image contributors. scikit-image: Image processing in Python. PeerJ 2:e453 (2014) http://dx.doi.org/10.7717/peerj.453\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
