{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMI 260 ASSIGNMENT #3 | Mammogram Spring 2018\n",
    "\n",
    "## Name 1:Joseph Nicolls\n",
    "\n",
    "## Name 2: Alex Lu\n",
    "\n",
    "Breast cancer has the highest incidence and second highest mortality rate for women in the US. \n",
    "Your task is to utilize machine learning to either classify AND/OR segment mammograms or neither, as long as you justify why it is useful to do whatever it is you want to do.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional ML techniques for Classification of Mammograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Background and Relevance\n",
    "\n",
    "Mammograms are difficult to classify because identifying features for malignant tumors can be vague and masses in mammograms can appear anywhere with any orientation in the breast tissue. Though the setup for this problem suggests use of CNNs, we want to use traditional ML techniques in this exploratory research for a variety of reasons. The principal reason that we want to do this is for the feature analysis possible with traditional ML techniques that isn't possible with CNNs. According to a study from Britton et. all, sensitivity of radiologists in classifying mammograms can vary between 53.1-74.1%. Through feature analysis, indicative features could be highlighted for radiologists to focus on, potentially raising sensitivities and decreasing the variability of sensitivities between radiologists. In essence, we will attempt to classify the malignancy of tumors within mammograms using traditional machine learning techniques based on quantitative features derived from the mammogram and categorical features provided by expert analysis on those mammograms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach and Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images were taken from the Digital Database for Screening Mammography (DDSM), a database which consists of 2620 mammogram studies. These images are X-rays, recorded as grey-scale images. he two classes are fairly balanced, with 52% benign mammograms and  48% malignant. \n",
    "\n",
    "The dataset also comes with a csv of precomputed features and a set of semantic descriptions. Previous work had been done to filter these images to the ROI mass. We will employ the masks to create several new features in addition to features that were previously computed.\n",
    "\n",
    "We have divided the data into training and testing at random, leaving 10% out for testing. T\n",
    "In addition, semantic features associated with some on the images have also been included. These semantic features include the type of view included in the image, which side of the patient the breast was on, and case descriptions with a limited vocabulary describing characteristics of the mass. We believe that semantic features such as breast density, mass shape, and mass margins can be highly useful. The limited vocabulary of these qualitative features allows one-hot encoding and incorporation into our feature vectors for traditional ML methods.\n",
    "Relevant statistics in our data are based on the conventional wisdom that malignant tumors typically have irregular borders and a non spherical shape. (Halls, Mammogram and Ultrasound Images Explained) In order to measure this, we compare the margins of the mass to its convex hull, or the smallest set of convex points which contains the ROI. Two statistics which provide a metric of this relationship is the convexity (the ratio of the convex hull perimeter to the ROI perimeter) and the solidity ( the ratio of the ROI area to the convex hull area.) The lower the convexity and lower the solidity, the less spherical the ROI is, potentially hinting at malignant character.\n",
    "In addition, we have also calculated a similar metric called extent. This metric is the ratio of the area of the ROI to the area of the bounding box of the ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try to leverage the following features in order to make attempts at classification: \n",
    "* features in the features_matrix.csv (ASM, Area, Centroid coordinates, ...) \n",
    "* convexity, solidity, and extent. (Calculated from masked ROI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decrease the feature dimensionality, we considered principal component analysis for feature extraction. In choosing to do so, we would have sacrificed variable interpretability. However, due to the relatively small feature space that we’re operating in, it’s not overly burdensome to trace backwards. In addition, identifying features that are strongly associated with each other could provide the ground for recommendations to pathologists. However, once we performed PCA, we found that one component accounted for 44% of the variability while all other principal components accounted for ~10%. In comparison to results from training features with normalization and zero-centering alone, PCA did not contribute significantly to model performance and only contributed to overfitting. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the packages that we're going to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "import os \n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.morphology import convex_hull_image\n",
    "from skimage import data, img_as_float\n",
    "from skimage.util import invert\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.measure import regionprops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, let's import from the other code files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train, labels):\n",
    "    \n",
    "    svcClf = SVC(random_state = 260)\n",
    "    svcClf.fit(train, labels)\n",
    "    \n",
    "    rfClf = RandomForestClassifier(random_state = 260)\n",
    "    rfClf.fit(train, labels)\n",
    "    \n",
    "    return [svcClf, rfClf]\n",
    "\n",
    "\n",
    "def Tuner(combined_features, groundtruth):\n",
    "    '''\n",
    "    Input: \n",
    "        * all features\n",
    "        * ground truth labels\n",
    "\n",
    "    Output:\n",
    "        * tuning grid of (accuracy, sensitivity, specificity) for specified bounds on C, class_weight\n",
    "\n",
    "    '''\n",
    "    C = .3\n",
    "    C_bound = 20\n",
    "    grid = np.zeros((C_bound, 3))\n",
    "    for i in range(C_bound):\n",
    "        grid[i] = EvaluateClassifier(combined_features, groundtruth, C)\n",
    "        C += .3\n",
    "    return grid\n",
    "\n",
    "\n",
    "def EvaluateClassifier(X, Y, C):\n",
    "    '''\n",
    "    Input: \n",
    "        * Dataset X\n",
    "        * Labels Y\n",
    "    Output:\n",
    "        * outputs performance metrics: accuracy, sens, spec\n",
    "\n",
    "    Evaluates a linear-kernel support classifer with given penalty input C and melanoma class-weight weight\n",
    "    with 10-fold cross-validation. We must adjust the class weight to account for the class imbalance inherent \n",
    "    to the dataset. \n",
    "    '''\n",
    "\n",
    "    svm_man = SVC(C = C, kernel='linear')\n",
    "\n",
    "    scores = cross_val_score(svm_man, X, Y, cv=10, scoring='accuracy')\n",
    "    acc = scores.mean()\n",
    "    print('acc is:')\n",
    "    print(acc)\n",
    "\n",
    "    #------ Compute metrics ---------\n",
    "    y_predict = cross_val_predict(svm_man, X, Y, cv=10)\n",
    "    conf_mat = confusion_matrix(Y, y_predict)\n",
    "    tn, fp, fn, tp = conf_mat.ravel()\n",
    "    sens = float(tp)/(tp+fn)\n",
    "    spec = float(tn)/(tn+fp)\n",
    "    print('sens is:')\n",
    "    print(sens)\n",
    "    print('spec is:')\n",
    "    print(spec)\n",
    "    print(conf_mat)\n",
    "\n",
    "    return (acc, sens, spec)\n",
    "\n",
    "\n",
    "def validate_models(models, test, y_true):\n",
    "    '''\n",
    "        input: \n",
    "            * models, a list of trained models \n",
    "            * test, the test set on which models will be evaluated\n",
    "            * feature_names, the list of feature columns that are being used \n",
    "    '''\n",
    "    print(\"Preparing to validate models\")\n",
    "    for model in models:\n",
    "        y_pred = model.predict(test)\n",
    "        conf_mat = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = conf_mat.ravel()\n",
    "        sens = float(tp)/(tp+fn)\n",
    "        spec = float(tn)/(tn+fp)\n",
    "        print('sens is:')\n",
    "        print(sens)\n",
    "        print('spec is:')\n",
    "        print(spec)\n",
    "        print(conf_mat)\n",
    "\n",
    "        print(\"auroc: \" +  str(roc_auc_score(y_true, y_pred)))\n",
    "        \n",
    "    print(\"Done validating models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_props(data): \n",
    "    ret,thresh_img = cv2.threshold(data,0,255,cv2.THRESH_BINARY)\n",
    "    thresh_props =  regionprops(thresh_img.astype(int))\n",
    "    thresh_area = thresh_props[0].area\n",
    "    thresh_perimeter = thresh_props[0].perimeter\n",
    "    chull = convex_hull_image(thresh_img)\n",
    "    props = regionprops(chull.astype(int))\n",
    "    chull_area = props[0].area\n",
    "    chull_perimeter = props[0].perimeter \n",
    "    convexity = chull_perimeter/thresh_perimeter\n",
    "    solidity = thresh_area/chull_area\n",
    "    return convexity, solidity, thresh_props[0].extent, thresh_props[0].major_axis_length, thresh_props[0].minor_axis_length\n",
    "    \n",
    "\n",
    "def reduce_dimensionality(raw_data, new_dims=3):\n",
    "    '''\n",
    "        input:\n",
    "            * raw_data, the raw matrix that will be reduced in dimensionality \n",
    "        output:\n",
    "            * the dimensionality-reduced data \n",
    "        \n",
    "    '''\n",
    "    print(\"preparing to reduce dimensionality\")\n",
    "    pca = PCA()\n",
    "    pca.fit(raw_data)\n",
    "    print(\">>> variance explained by each principal component\")\n",
    "    print(pca.explained_variance_ratio_)  \n",
    "    print(\">>> the first principal component\")\n",
    "    print(pca.components_[0])\n",
    "    reduced = pca.transform(raw_data)[:,:new_dims]\n",
    "    return reduced\n",
    "\n",
    "\n",
    "def mean_center_normalize(data):\n",
    "    data -= np.mean(data, axis = 0)\n",
    "    data /= np.std(data, axis = 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_images(data_parent_dir):\n",
    "    '''\n",
    "        input:\n",
    "            * paths, a list of the paths that we need to input \n",
    "        output:\n",
    "            * a dataframe containing image data, label, and name \n",
    "    '''\n",
    "    print(\"preparing to read images\")\n",
    "    \n",
    "    # initialize data structures \n",
    "    data = []\n",
    "    label = []\n",
    "    name = []\n",
    "    convexities = []\n",
    "    solidities = []\n",
    "    extents = []\n",
    "    major_axis_lengths = []\n",
    "    minor_axis_lengths = []\n",
    "    \n",
    "    paths = os.listdir(data_parent_dir)\n",
    "\n",
    "    scan_ids = []\n",
    "    \n",
    "    # iteration over paths, dirs\n",
    "    for first_path in paths:\n",
    "        if first_path[0] == '.': # check for random dot files that come up :( \n",
    "            continue\n",
    "        local_dir = os.path.join(data_parent_dir, first_path)\n",
    "        for image in os.listdir(local_dir):\n",
    "            scan_ids.append(\"_\".join((first_path, image[:-3])))\n",
    "            with h5py.File(os.path.join(local_dir, image), 'r') as hf:\n",
    "                data.append(np.array(hf.get('data')))\n",
    "                label.append(np.array(hf.get('label')).item(0))\n",
    "                name.append(np.array(hf.get('name')))\n",
    "                \n",
    "                # compute additional features\n",
    "                convexity, solidity, extent, major_axis_length, minor_axis_length = get_props(hf.get('data')[:, :, 1])\n",
    "                convexities.append(convexity)\n",
    "                solidities.append(solidity)\n",
    "                extents.append(extent)\n",
    "                major_axis_lengths.append(major_axis_length)\n",
    "                minor_axis_lengths.append(minor_axis_length)\n",
    "         \n",
    "    print(scan_ids[:10])\n",
    "    scan_ids = [\"P_\" +scan_id for scan_id in scan_ids]\n",
    "    \n",
    "    d = {'pixel_data':data, 'label':label, 'name':name}\n",
    "    \n",
    "    d_computed = {'convexity': convexities, 'solidity': solidities, 'extent': extents, 'major_axis_length': major_axis_lengths, 'minor_axis_length': minor_axis_lengths}\n",
    "   \n",
    "    df_img = pd.DataFrame(data=d, index=scan_ids)\n",
    "    df_computed = pd.DataFrame(data=d_computed, index=scan_ids)\n",
    "    \n",
    "    return (df_img, df_computed, scan_ids)\n",
    "\n",
    "\n",
    "def drop_excess_rows(scan_ids, precomputed_df):\n",
    "\n",
    "    drop_list = []\n",
    "    true_list = []\n",
    "    scan_ids = set(scan_ids)\n",
    "    names = precomputed_df.index.values\n",
    "    for name in names:\n",
    "        if name not in scan_ids:\n",
    "            drop_list.append(name)\n",
    "\n",
    "    print(\"the drop list has: \" + str(len(drop_list)))\n",
    "    return precomputed_df.drop(drop_list)\n",
    "\n",
    "\n",
    "def gather_semantic_features(semantics_path):\n",
    "    semantic_df = pd.read_csv(semantics_path)\n",
    "    semantic_df.dropna(inplace=True)\n",
    "    return semantic_df\n",
    "\n",
    "\n",
    "def encode_categorical_labels(semantic_df, semantic_feature_names):\n",
    "    '''\n",
    "        input:\n",
    "        \n",
    "        output:\n",
    "        \n",
    "    '''\n",
    "    for feature in semantic_feature_names:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(list(semantic_df[feature].astype(str)))\n",
    "        semantic_df[feature] = le.transform(semantic_df[feature])\n",
    "    return semantic_df\n",
    "\n",
    "\n",
    "def one_hot_encoding(semantic_df, semantic_feature_names):\n",
    "    '''\n",
    "        input:\n",
    "        \n",
    "        output:\n",
    "        \n",
    "    '''\n",
    "    enc = preprocessing.OneHotEncoder(sparse=False)\n",
    "    semantic_one_hots = enc.fit_transform(semantic_df[semantic_feature_names])\n",
    "    _, one_hot_length = semantic_one_hots.shape\n",
    "    return (semantic_one_hots, one_hot_length)\n",
    "\n",
    "\n",
    "def generate_semantic_df(semantics_path, semantic_feature_names, total_patientIDs):\n",
    "\t'''\n",
    "\n",
    "\t\tHere, we find that there are semantic descriptions of images that do not\n",
    "\t\tappear in the h5'd dataset, and also images that appear in this dataset \n",
    "\t\twithout corresponding semantic descriptions.  \n",
    "\t'''\n",
    "    \n",
    "\tsemantic_df = gather_semantic_features(semantics_path)\n",
    "\tsemantic_df = encode_categorical_labels(semantic_df, semantic_feature_names)\n",
    "\tsemantic_one_hots, one_hot_length = one_hot_encoding(semantic_df, semantic_feature_names)\n",
    "\n",
    "\thas_semantic = [s1 + \"_\" + s2 + \"_\" + s3 for (s1, s2, s3) in zip(list(semantic_df['patient_id']), list(semantic_df['side']), list(semantic_df['view']))]\n",
    "\n",
    "\tsemantic_encoded_dict = {img:np.zeros(one_hot_length) for (idx, img) in enumerate(total_patientIDs)}\n",
    "\n",
    "\tfor img, patient_id in enumerate(has_semantic): \n",
    "\t\tif patient_id in semantic_encoded_dict.keys():\n",
    "\t\t\tsemantic_encoded_dict[patient_id] = semantic_one_hots[img] \n",
    "\n",
    "\tprint('---')\n",
    "\tprint(len(has_semantic))\n",
    "\tprint(one_hot_length)\n",
    "\tprint(len(total_patientIDs))\n",
    "\tprint(len(semantic_encoded_dict.keys()))\n",
    "\n",
    "\tencoded_feature_names = [\"one_hot #\" + str(x) for x in range(1, one_hot_length+1)]\n",
    "\tsemantic_encoded_df = pd.DataFrame.from_dict(semantic_encoded_dict, orient='index', columns=encoded_feature_names)\n",
    "\tprint(semantic_encoded_df.index)\n",
    "\t#    semantic_encoded_df.set_index(total_patientIDs)\n",
    "\treturn (semantic_encoded_df, encoded_feature_names)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially predict some potential issues with this approach: the number of features is large, and many are potentially correlated. For that reason, we can implement some dimensionality reduction and take a look at the variation explained by the principal components (and their contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginnings of our preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_parent_dir, precomputed_path, dim_reduc=None):\n",
    "    print('Entering preprocessing')\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    # reading things in \n",
    "    image_df, computed_df, scan_ids = gather_images(data_parent_dir)\n",
    "    precomputed_df = drop_excess_rows(scan_ids, pd.read_csv(precomputed_path, index_col=0))\n",
    "\n",
    "    # an initial merge\n",
    "    feature_df = pd.merge(computed_df, precomputed_df, on=precomputed_df.index)\n",
    "    \n",
    "    # zero-centering, normalization of all data within the feature dataframe \n",
    "    features = feature_df.values[:,1:]\n",
    "    print(features.shape)\n",
    "    new_fts = mean_center_normalize(np.array(features,dtype=np.float32))\n",
    "    print(\"==============================\")\n",
    "    print(new_fts.shape)\n",
    "    \n",
    "    print(image_df.describe())\n",
    "    print(computed_df.describe())\n",
    "    print(precomputed_df.describe())\n",
    "    \n",
    "    # dimensionality reduction \n",
    "    if dim_reduc is not None:\n",
    "        new_fts = reduce_dimensionality(new_fts, dim_reduc)\n",
    "        feature_names += [\"pc# \"+str(x) for x in range(1, dim_reduc+1)]\n",
    "        feature_df = pd.DataFrame(new_fts, columns=feature_names, index=precomputed_df.index)\n",
    "        print(\"==============================\")\n",
    "        print(feature_df.info())\n",
    "        \n",
    "    # adding back to df \n",
    "    else:\n",
    "        feature_names += list(feature_df)[1:]\n",
    "        feature_df[feature_names] = new_fts\n",
    "        print(\"==============================\")\n",
    "        print(feature_df.info())\n",
    "    \n",
    "    print(feature_names)\n",
    "    \n",
    "    # joining dfs\n",
    "    df = pd.merge(image_df, feature_df[feature_names], on=feature_df.index)\n",
    "    df.info()\n",
    "    \n",
    "    train, test = train_test_split(df, test_size=0.2, random_state=260)\n",
    "    \n",
    "    return (train[feature_names], train['label'], test[feature_names], test['label'], scan_ids)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've pretty much established how our data should be preprocessed, we want to introduce and train some basic machine learning models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parent_dir = \"./data_fixed_crop_w_mask\"\n",
    "precomputed_path = \"features_matrix.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering preprocessing\n",
      "preparing to read images\n",
      "['00001_LEFT_CC', '00001_LEFT_MLO', '00004_LEFT_CC', '00004_LEFT_MLO', '00004_RIGHT_CC', '00004_RIGHT_MLO', '00009_RIGHT_CC', '00009_RIGHT_MLO', '00015_LEFT_MLO', '00016_LEFT_CC']\n",
      "the drop list has: 91\n",
      "(1508, 23)\n",
      "==============================\n",
      "(1508, 23)\n",
      "             label\n",
      "count  1508.000000\n",
      "mean      0.478780\n",
      "std       0.499715\n",
      "min       0.000000\n",
      "25%       0.000000\n",
      "50%       0.000000\n",
      "75%       1.000000\n",
      "max       1.000000\n",
      "         convexity     solidity       extent  major_axis_length  \\\n",
      "count  1508.000000  1508.000000  1508.000000        1508.000000   \n",
      "mean      0.823225     0.863078     0.644197          90.195399   \n",
      "std       0.151674     0.117002     0.107757          62.893666   \n",
      "min       0.461828     0.047736     0.016178          31.827394   \n",
      "25%       0.754172     0.853156     0.620854          55.699932   \n",
      "50%       0.821002     0.885096     0.662134          68.380669   \n",
      "75%       0.876884     0.912385     0.699197          93.923356   \n",
      "max       3.014770     0.985606     0.873839         589.834634   \n",
      "\n",
      "       minor_axis_length  \n",
      "count        1508.000000  \n",
      "mean           70.213958  \n",
      "std            40.035041  \n",
      "min            23.460490  \n",
      "25%            45.367231  \n",
      "50%            57.932467  \n",
      "75%            77.418287  \n",
      "max           247.177242  \n",
      "               ASM           Area   Centroid_x   Centroid_y     Contrast  \\\n",
      "count  1508.000000    1508.000000  1508.000000  1508.000000  1508.000000   \n",
      "mean      0.989816   82196.687666     0.509939     0.516854     0.002132   \n",
      "std       0.009185   72112.671733     0.155333     0.245478     0.001284   \n",
      "min       0.905100    4567.000000     0.093025     0.019573     0.000200   \n",
      "25%       0.987500   38052.750000     0.392484     0.325622     0.001200   \n",
      "50%       0.992500   59975.000000     0.509351     0.522706     0.001900   \n",
      "75%       0.995700  100590.500000     0.624067     0.722545     0.002600   \n",
      "max       0.999200  656650.000000     0.923238     0.986207     0.009100   \n",
      "\n",
      "       Dissimilarity  Eccentricity       Energy  Homogeneity  Max intensity  \\\n",
      "count    1508.000000   1508.000000  1508.000000  1508.000000    1508.000000   \n",
      "mean        0.000978      0.569280     0.994883     0.999626     188.931134   \n",
      "std         0.000613      0.165532     0.004645     0.000258      31.375588   \n",
      "min         0.000100      0.149002     0.951300     0.997500      56.377432   \n",
      "25%         0.000600      0.459396     0.993700     0.999500     171.147860   \n",
      "50%         0.000800      0.570317     0.996300     0.999700     191.642023   \n",
      "75%         0.001200      0.671370     0.997800     0.999800     210.673617   \n",
      "max         0.005100      0.999369     0.999600     1.000000     255.000000   \n",
      "\n",
      "       Mean intensity  Min intensity  Orientation    Perimeter  \\\n",
      "count     1508.000000    1508.000000  1508.000000  1508.000000   \n",
      "mean       150.672146     102.798374     0.025581  1880.514226   \n",
      "std         32.871146      40.443288     0.890460   983.778491   \n",
      "min         29.622591       0.000000    -1.570040   370.143182   \n",
      "25%        131.409523      80.486381    -0.618421  1218.628859   \n",
      "50%        153.473748     108.038911     0.020111  1622.937300   \n",
      "75%        173.558266     131.247082     0.678797  2209.163743   \n",
      "max        231.298409     203.706897     1.570796  8591.265146   \n",
      "\n",
      "       Relative intensity     Solidity  StdDev intensity  Texture correlation  \n",
      "count         1508.000000  1508.000000       1508.000000          1508.000000  \n",
      "mean             0.740981     0.834554         10.139930             0.962781  \n",
      "std              0.123214     0.118032          5.354910             0.013902  \n",
      "min              0.221088     0.038955          0.982200             0.875400  \n",
      "25%              0.671168     0.817636          6.313000             0.955500  \n",
      "50%              0.751537     0.855264          9.042100             0.964800  \n",
      "75%              0.826171     0.886733         12.763475             0.972300  \n",
      "max              1.063823     0.989777         35.282200             0.994700  \n",
      "==============================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1508 entries, 0 to 1507\n",
      "Data columns (total 24 columns):\n",
      "key_0                  1508 non-null object\n",
      "convexity              1508 non-null float64\n",
      "solidity               1508 non-null float64\n",
      "extent                 1508 non-null float64\n",
      "major_axis_length      1508 non-null float64\n",
      "minor_axis_length      1508 non-null float64\n",
      "ASM                    1508 non-null float64\n",
      "Area                   1508 non-null float64\n",
      "Centroid_x             1508 non-null float64\n",
      "Centroid_y             1508 non-null float64\n",
      "Contrast               1508 non-null float64\n",
      "Dissimilarity          1508 non-null float64\n",
      "Eccentricity           1508 non-null float64\n",
      "Energy                 1508 non-null float64\n",
      "Homogeneity            1508 non-null float64\n",
      "Max intensity          1508 non-null float64\n",
      "Mean intensity         1508 non-null float64\n",
      "Min intensity          1508 non-null float64\n",
      "Orientation            1508 non-null float64\n",
      "Perimeter              1508 non-null float64\n",
      "Relative intensity     1508 non-null float64\n",
      "Solidity               1508 non-null float64\n",
      "StdDev intensity       1508 non-null float64\n",
      "Texture correlation    1508 non-null float64\n",
      "dtypes: float64(23), object(1)\n",
      "memory usage: 294.5+ KB\n",
      "None\n",
      "['convexity', 'solidity', 'extent', 'major_axis_length', 'minor_axis_length', 'ASM', 'Area', 'Centroid_x', 'Centroid_y', 'Contrast', 'Dissimilarity', 'Eccentricity', 'Energy', 'Homogeneity', 'Max intensity', 'Mean intensity', 'Min intensity', 'Orientation', 'Perimeter', 'Relative intensity', 'Solidity', 'StdDev intensity', 'Texture correlation']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1508 entries, 0 to 1507\n",
      "Data columns (total 27 columns):\n",
      "key_0                  1508 non-null int64\n",
      "pixel_data             1508 non-null object\n",
      "label                  1508 non-null int64\n",
      "name                   1508 non-null object\n",
      "convexity              1508 non-null float64\n",
      "solidity               1508 non-null float64\n",
      "extent                 1508 non-null float64\n",
      "major_axis_length      1508 non-null float64\n",
      "minor_axis_length      1508 non-null float64\n",
      "ASM                    1508 non-null float64\n",
      "Area                   1508 non-null float64\n",
      "Centroid_x             1508 non-null float64\n",
      "Centroid_y             1508 non-null float64\n",
      "Contrast               1508 non-null float64\n",
      "Dissimilarity          1508 non-null float64\n",
      "Eccentricity           1508 non-null float64\n",
      "Energy                 1508 non-null float64\n",
      "Homogeneity            1508 non-null float64\n",
      "Max intensity          1508 non-null float64\n",
      "Mean intensity         1508 non-null float64\n",
      "Min intensity          1508 non-null float64\n",
      "Orientation            1508 non-null float64\n",
      "Perimeter              1508 non-null float64\n",
      "Relative intensity     1508 non-null float64\n",
      "Solidity               1508 non-null float64\n",
      "StdDev intensity       1508 non-null float64\n",
      "Texture correlation    1508 non-null float64\n",
      "dtypes: float64(23), int64(2), object(2)\n",
      "memory usage: 329.9+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test, all_IDs = preprocess(data_parent_dir, precomputed_path, dim_reduc=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models and Results\n",
    "\n",
    "For this exploratory research, we examined SVMs and Random Forests. SVMs were a natural choice due to our small feature space and even class distribution. In addition, Random Forest models have the potential to elucidate unconventional relationships between seemingly independent variables. Due to our use of SVMs, normalization was required to prevent features with significantly high amplitudes from dominating the prediction outcome. \n",
    "\n",
    "From the features that were previously computed, performance in these models was worse than the majority classifier. However, with the addition of our computed features, we were able to improve the performance of our classifier measurably. Before tuning, our SVM performed with a sensitivity of .49 and a specificity of .82. Our RF performed with a sensitivity of .57 and a specificity of .75. Our area under the receiver operator curve for our SVM model and RF model was .65 and .66 respectively. In order to improve these metrics, we performed hyperparameter tuning. In out tuning, we discovered that a major measure that our models fell behind on was in sensitivity. This makes sense, as malicious tumors are easier to spot in a mammogram, but it is more difficult to tell the difference, based on outline alone, to tell the difference between a benign and malicious mass. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to validate models\n",
      "sens is:\n",
      "0.4697986577181208\n",
      "spec is:\n",
      "0.6862745098039216\n",
      "[[105  48]\n",
      " [ 79  70]]\n",
      "auroc: 0.5780365837610212\n",
      "sens is:\n",
      "0.3422818791946309\n",
      "spec is:\n",
      "0.6666666666666666\n",
      "[[102  51]\n",
      " [ 98  51]]\n",
      "auroc: 0.5044742729306488\n",
      "Done validating models\n"
     ]
    }
   ],
   "source": [
    "models = train_models(X_train, y_train)\n",
    "validate_models(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've seen our models perform like hot garbage, we're going to try to add in the semantic feature encoding to see if that will help "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess2(data_parent_dir, precomputed_path, semantics_path, semantic_feature_names, dim_reduc=None):\n",
    "    print('Entering preprocessing')\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    # reading things in \n",
    "    image_df, computed_df, scan_ids = gather_images(data_parent_dir)\n",
    "    precomputed_df = drop_excess_rows(scan_ids, pd.read_csv(precomputed_path, index_col=0))\n",
    "\n",
    "    # an initial merge\n",
    "    feature_df = pd.merge(computed_df, precomputed_df, on=precomputed_df.index)\n",
    "    \n",
    "    # zero-centering, normalization of all data within the feature dataframe \n",
    "    features = feature_df.values[:,1:]\n",
    "    new_fts = mean_center_normalize(np.array(features,dtype=np.float32))\n",
    "    \n",
    "    # dimensionality reduction \n",
    "    if dim_reduc is not None:\n",
    "        new_fts = reduce_dimensionality(new_fts, dim_reduc)\n",
    "        feature_names += [\"pc# \"+str(x) for x in range(1, dim_reduc+1)]\n",
    "        feature_df = pd.DataFrame(new_fts, columns=feature_names, index=precomputed_df.index)\n",
    "\n",
    "\n",
    "        \n",
    "    # adding back to df \n",
    "    else:\n",
    "        feature_names += list(feature_df)[1:]\n",
    "        feature_df[feature_names] = new_fts\n",
    "    \n",
    "    \n",
    "    # get the semantic df\n",
    "    semantic_df, encoded_feature_names = generate_semantic_df(semantics_path, semantic_feature_names, scan_ids)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # joining dfs\n",
    "    df = pd.merge(image_df, feature_df[feature_names], on=feature_df.index)\n",
    "    print(df.index)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.set_index(image_df.index, inplace=True)\n",
    "    #print(semantic_df.info())\n",
    "    all_features_df = pd.merge(df, semantic_df, left_index=True, right_index=True, sort=False)\n",
    "    all_features_df.info()\n",
    "    \n",
    "    feature_names += encoded_feature_names\n",
    "    train, test = train_test_split(all_features_df, test_size=0.2, random_state=260)\n",
    "    \n",
    "    return (train[feature_names], train['label'], test[feature_names], test['label'], scan_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering preprocessing\n",
      "preparing to read images\n",
      "['00001_LEFT_CC', '00001_LEFT_MLO', '00004_LEFT_CC', '00004_LEFT_MLO', '00004_RIGHT_CC', '00004_RIGHT_MLO', '00009_RIGHT_CC', '00009_RIGHT_MLO', '00015_LEFT_MLO', '00016_LEFT_CC']\n",
      "the drop list has: 91\n",
      "---\n",
      "1273\n",
      "49\n",
      "1508\n",
      "1508\n",
      "Index(['P_00001_LEFT_CC', 'P_00001_LEFT_MLO', 'P_00004_LEFT_CC',\n",
      "       'P_00004_LEFT_MLO', 'P_00004_RIGHT_CC', 'P_00004_RIGHT_MLO',\n",
      "       'P_00009_RIGHT_CC', 'P_00009_RIGHT_MLO', 'P_00015_LEFT_MLO',\n",
      "       'P_00016_LEFT_CC',\n",
      "       ...\n",
      "       'P_01550_RIGHT_CC', 'P_01550_RIGHT_MLO', 'P_01551_LEFT_CC',\n",
      "       'P_01551_LEFT_MLO', 'P_01553_RIGHT_CC', 'P_01553_RIGHT_MLO',\n",
      "       'P_01555_LEFT_MLO', 'P_01556_LEFT_CC', 'P_01557_RIGHT_CC',\n",
      "       'P_01557_RIGHT_MLO'],\n",
      "      dtype='object', length=1508)\n",
      "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "            ...\n",
      "            1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507],\n",
      "           dtype='int64', length=1508)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1508 entries, P_00001_LEFT_CC to P_01557_RIGHT_MLO\n",
      "Data columns (total 76 columns):\n",
      "key_0                  1508 non-null int64\n",
      "pixel_data             1508 non-null object\n",
      "label                  1508 non-null int64\n",
      "name                   1508 non-null object\n",
      "convexity              1508 non-null float64\n",
      "solidity               1508 non-null float64\n",
      "extent                 1508 non-null float64\n",
      "major_axis_length      1508 non-null float64\n",
      "minor_axis_length      1508 non-null float64\n",
      "ASM                    1508 non-null float64\n",
      "Area                   1508 non-null float64\n",
      "Centroid_x             1508 non-null float64\n",
      "Centroid_y             1508 non-null float64\n",
      "Contrast               1508 non-null float64\n",
      "Dissimilarity          1508 non-null float64\n",
      "Eccentricity           1508 non-null float64\n",
      "Energy                 1508 non-null float64\n",
      "Homogeneity            1508 non-null float64\n",
      "Max intensity          1508 non-null float64\n",
      "Mean intensity         1508 non-null float64\n",
      "Min intensity          1508 non-null float64\n",
      "Orientation            1508 non-null float64\n",
      "Perimeter              1508 non-null float64\n",
      "Relative intensity     1508 non-null float64\n",
      "Solidity               1508 non-null float64\n",
      "StdDev intensity       1508 non-null float64\n",
      "Texture correlation    1508 non-null float64\n",
      "one_hot #1             1508 non-null float64\n",
      "one_hot #2             1508 non-null float64\n",
      "one_hot #3             1508 non-null float64\n",
      "one_hot #4             1508 non-null float64\n",
      "one_hot #5             1508 non-null float64\n",
      "one_hot #6             1508 non-null float64\n",
      "one_hot #7             1508 non-null float64\n",
      "one_hot #8             1508 non-null float64\n",
      "one_hot #9             1508 non-null float64\n",
      "one_hot #10            1508 non-null float64\n",
      "one_hot #11            1508 non-null float64\n",
      "one_hot #12            1508 non-null float64\n",
      "one_hot #13            1508 non-null float64\n",
      "one_hot #14            1508 non-null float64\n",
      "one_hot #15            1508 non-null float64\n",
      "one_hot #16            1508 non-null float64\n",
      "one_hot #17            1508 non-null float64\n",
      "one_hot #18            1508 non-null float64\n",
      "one_hot #19            1508 non-null float64\n",
      "one_hot #20            1508 non-null float64\n",
      "one_hot #21            1508 non-null float64\n",
      "one_hot #22            1508 non-null float64\n",
      "one_hot #23            1508 non-null float64\n",
      "one_hot #24            1508 non-null float64\n",
      "one_hot #25            1508 non-null float64\n",
      "one_hot #26            1508 non-null float64\n",
      "one_hot #27            1508 non-null float64\n",
      "one_hot #28            1508 non-null float64\n",
      "one_hot #29            1508 non-null float64\n",
      "one_hot #30            1508 non-null float64\n",
      "one_hot #31            1508 non-null float64\n",
      "one_hot #32            1508 non-null float64\n",
      "one_hot #33            1508 non-null float64\n",
      "one_hot #34            1508 non-null float64\n",
      "one_hot #35            1508 non-null float64\n",
      "one_hot #36            1508 non-null float64\n",
      "one_hot #37            1508 non-null float64\n",
      "one_hot #38            1508 non-null float64\n",
      "one_hot #39            1508 non-null float64\n",
      "one_hot #40            1508 non-null float64\n",
      "one_hot #41            1508 non-null float64\n",
      "one_hot #42            1508 non-null float64\n",
      "one_hot #43            1508 non-null float64\n",
      "one_hot #44            1508 non-null float64\n",
      "one_hot #45            1508 non-null float64\n",
      "one_hot #46            1508 non-null float64\n",
      "one_hot #47            1508 non-null float64\n",
      "one_hot #48            1508 non-null float64\n",
      "one_hot #49            1508 non-null float64\n",
      "dtypes: float64(72), int64(2), object(2)\n",
      "memory usage: 947.2+ KB\n"
     ]
    }
   ],
   "source": [
    "data_parent_dir = \"./data_fixed_crop_w_mask\"\n",
    "precomputed_path = \"features_matrix.csv\"\n",
    "semantics_path = \"mass_case_description_train_set.csv\"\n",
    "\n",
    "semantic_feature_names = ['breast_density', 'abn_num', 'mass_shape', 'mass_margins', 'assessment']\n",
    "\n",
    "X_train, y_train, X_test, y_test, scan_ids = preprocess2(data_parent_dir, \n",
    "                                                       precomputed_path, \n",
    "                                                         semantics_path,\n",
    "                                                       semantic_feature_names,\n",
    "                                                       dim_reduc=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to validate models\n",
      "sens is:\n",
      "0.4899328859060403\n",
      "spec is:\n",
      "0.8235294117647058\n",
      "[[126  27]\n",
      " [ 76  73]]\n",
      "auroc: 0.6567311488353731\n",
      "sens is:\n",
      "0.5771812080536913\n",
      "spec is:\n",
      "0.7581699346405228\n",
      "[[116  37]\n",
      " [ 63  86]]\n",
      "auroc: 0.6676755713471071\n",
      "Done validating models\n"
     ]
    }
   ],
   "source": [
    "models = train_models(X_train, y_train)\n",
    "validate_models(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc is:\n",
      "0.7371857923497268\n",
      "sens is:\n",
      "0.5654450261780105\n",
      "spec is:\n",
      "0.8925750394944708\n",
      "[[565  68]\n",
      " [249 324]]\n",
      "acc is:\n",
      "0.7421584699453552\n",
      "sens is:\n",
      "0.5706806282722513\n",
      "spec is:\n",
      "0.8973143759873617\n",
      "[[568  65]\n",
      " [246 327]]\n",
      "acc is:\n",
      "0.7438114754098359\n",
      "sens is:\n",
      "0.5741710296684118\n",
      "spec is:\n",
      "0.8973143759873617\n",
      "[[568  65]\n",
      " [244 329]]\n",
      "acc is:\n",
      "0.7429918032786885\n",
      "sens is:\n",
      "0.5706806282722513\n",
      "spec is:\n",
      "0.8988941548183255\n",
      "[[569  64]\n",
      " [246 327]]\n",
      "acc is:\n",
      "0.7446311475409836\n",
      "sens is:\n",
      "0.5724258289703316\n",
      "spec is:\n",
      "0.9004739336492891\n",
      "[[570  63]\n",
      " [245 328]]\n",
      "acc is:\n",
      "0.7413114754098361\n",
      "sens is:\n",
      "0.5724258289703316\n",
      "spec is:\n",
      "0.8941548183254344\n",
      "[[566  67]\n",
      " [245 328]]\n",
      "acc is:\n",
      "0.7396448087431695\n",
      "sens is:\n",
      "0.5724258289703316\n",
      "spec is:\n",
      "0.8909952606635071\n",
      "[[564  69]\n",
      " [245 328]]\n",
      "acc is:\n",
      "0.7396584699453552\n",
      "sens is:\n",
      "0.5724258289703316\n",
      "spec is:\n",
      "0.8909952606635071\n",
      "[[564  69]\n",
      " [245 328]]\n",
      "acc is:\n",
      "0.7396584699453552\n",
      "sens is:\n",
      "0.5724258289703316\n",
      "spec is:\n",
      "0.8909952606635071\n",
      "[[564  69]\n",
      " [245 328]]\n",
      "acc is:\n",
      "0.7404781420765028\n",
      "sens is:\n",
      "0.5724258289703316\n",
      "spec is:\n",
      "0.8925750394944708\n",
      "[[565  68]\n",
      " [245 328]]\n"
     ]
    }
   ],
   "source": [
    "grid = Tuner(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VisualizeTuning(df, measure, vmin=None, vmax=None):\n",
    "\n",
    "    (X_low, X_high, X_step) = X_bounds\n",
    "  \n",
    "    ax = sns.heatmap(df, \n",
    "                     vmin=vmin, \n",
    "                     vmax=vmax,\n",
    "                     cmap=\"YlGnBu\")\n",
    "#     plt.xlabel('Malignant Lesion Class Weight')\n",
    "#     plt.ylabel('C')\n",
    "    #plt.colorbar()\n",
    "    #g.axes.xticks(np.arange(8), np.arange(X_low, X_high, X_step))\n",
    "    #g.axes.yticks(np.arange(8), np.arange(Y_low, Y_high, Y_step))\n",
    "    ax.set_title('Exploration -- 10-fold CV %s'%measure)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = [str(.3 + a*.3) for a in range(20)]\n",
    "graph_labels = ['Accuracy', 'Sensitivity', 'Specificity']\n",
    "dfs = [pd.DataFrame({graph_labels[i]:grid[:,i]}, index = index) for i in range(3)]\n",
    "\n",
    "for i in range(3):\n",
    "    plt.figure()\n",
    "    ax = sns.heatmap(dfs[i],  cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Work\n",
    "\n",
    "So, it turns out that the semantic features do a little something; unfortunately, we didn't have a lot of opportunities to tune the classifer or do more legit feature engineering, but it's been really exciting to explore the different features and approaches that we can take, even if we're not doing something flashy like CNNs. \n",
    "\n",
    "As mentioned in our introduction, vanilla ML is significantly more interpretable and parsable in terms of the importance of specific variables. In addition, vanilla ML allows us to incorporate expert opinion and analysis into our models. Because of this, our models do not need to learn specific relationships between masses and malignant character that is already common domain knowledge; most features focus on the irregularity of the mass's borders and deviance from sphericality. \n",
    "The interpretability of our vanilla ML models allowed us to draw several conclusions as a result of our research. Though we did not incorporate this into our final model, our exploratory research into PCA demonstrated the dominance of a few features in the variability of our model’s predictions. In addition, our hyperparameter tuning allowed us to formalize a commonly known wisdom about mammogram analysis; it is a difficult problem to differentiate between benign and malicious masses. \n",
    "\n",
    "\n",
    "For future work, more feature analysis could potentially result in useful findings and recommendations for pathologists who are examining mammograms. In addition, it would be worthwhile to compare the performance on this model to a CNN to see which images can be misclassified due to dependence on conventional metrics. Finding examples of such images that are successfully classified under CNN but not not correctly classified under vanilla ML approaches would provide excellent examples to provide to pathologists to warn them about the unconventional nature of different types of masses. In addition, due to both lack of time and the limited scope of the data provided, we were unable to to train models based on different orientations of mages. Training different models for different orientations could provide useful feedback on what variables are more indicative of a mass's malignant character in different kinds of images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Bradski, G. (2000). The OpenCV Library. Dr. Dobb's Journal of Software Tools\n",
    "\n",
    "Britton P, Warwick J, Wallis MG, et al. Measuring the accuracy of diagnostic imaging in symptomatic breast patients: team and individual performance. The British Journal of Radiology. 2012;85(1012):415-422. doi:10.1259/bjr/32906819.\n",
    "\n",
    "Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "\n",
    "Stéfan van der Walt, Johannes L. Schönberger, Juan Nunez-Iglesias, François Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu and the scikit-image contributors. scikit-image: Image processing in Python. PeerJ 2:e453 (2014) http://dx.doi.org/10.7717/peerj.453\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
