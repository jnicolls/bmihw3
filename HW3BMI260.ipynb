{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMI 260 ASSIGNMENT #3 | Mammogram Spring 2018\n",
    "\n",
    "## Name 1:Joseph Nicolls\n",
    "\n",
    "## Name 2: Alex Lu\n",
    "\n",
    "Breast cancer has the highest incidence and second highest mortality rate for women in the US. \n",
    "Your task is to utilize machine learning to either classify AND/OR segment mammograms or neither, as long as you justify why it is useful to do whatever it is you want to do. If someone turns in a deep dream assignment using mammograms this might be amusing, but not so useful to patients. Consider this a mini-project, we highly suggest you work with 1 other person, it can be someone in your team. \n",
    "\n",
    "In addition to the mammograms, the dataset includes segmentations and mass_case_description_train_set.csv, which contains information about the mass shape, mass margins, assessment number, pathology diagnosis, and subtlety. Take some time to research what all of these different fields mean and whether you can use them in your algorithm or not. You dont need to use all of what is provided to you. \n",
    "\n",
    "Some ideas:\n",
    "\n",
    "1. use the ROIâ€™s or segmentations to extract features, and then train a classifier based on those features using the algorithms presented to you in the machine learning lectures, does not need to be deep learning. \n",
    "\n",
    "2. use convolutional neural networks, feel free to use any of the code we went over in class or use your own (custom code, sklearn, keras, Tensorflow etc.). If you dont want to place helper functions and classes into this notebook, place them in a .py file in the same folder called helperfunctions.py and import them into this notebook. \n",
    "\n",
    "The data is here:\n",
    "\n",
    "https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM\n",
    "\n",
    "If you do not like python, you can use a different language and turn your assignment in as a folder with all your code, a folder with all your figures and a latex or doc file with the writeup. The writeup doesnt need to be long, 1 page will do and cite at least one clinical paper and one technical paper. If you like python please place the writeup and code into this notebook. Use the markdown to tell is what you are doing in each section. You will not be graded on the performance of your model, only the scientific soundness of your claims, methodology, evaluation (ie fair but insightful statistics) and discussion of the shortcomings of what you tried. \n",
    "\n",
    "The format for this homework is as follows:\n",
    "\n",
    "1. Describe what you are doing and why it matters to patients using at least one citation\n",
    "\n",
    "2. Describe the relevant statistics of the data, how were the images taken ? how were they labeled ? what is the class balance and majority classifier accuracy ? How will you divide the data into testing, training and validation.\n",
    "\n",
    "3. Describe your data pipeline, ie how is the data scrubbed, normalized, stored and fed to the model for training. \n",
    "\n",
    "4. Explain how the model you chose works along side the code for it and at least one technical citation to give credit where credit is due. \n",
    "\n",
    "5. There are many ways to do training, take us through how you do it. (ie we used early stopping and we decided when to stop based on validation loss)  \n",
    "\n",
    "6. Make a figure displaying your results\n",
    "\n",
    "7. Discuss pros and cons of your method and what you might have done differently now that youve tried or would try if you had more time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional ML techniques for Classification of Mammograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Approach and Relevance\n",
    "\n",
    "Mammograms are difficult to classify because identifying features for malignant tumors can be vague and masses in mammograms can appear anywhere with any orientation in the breast tissue. Though the setup for this problem suggests use of CNNs, we want to use traditional ML techniques in this exploratory research for a variety of reasons. The principal reason that we want to do this is for the feature analysis possible with traditional ML technqiues that isn't possible with CNNs. According to a study from Britton et. all, sensitivity of radiologists in classifying mammograms can vary between 53.1-74.1%. Through feature analysis, indiciative features could be highlighted for radiologists to focus on, potentially raising sensitivites and decreasing the variability of senestivities between radiologists. In essence, we will attempt to classify the malginancy of tumors within mammograms using traditional machine learning techniques based on quanitative features derived from the mammogram and categorical features provided by expert analysis on those mammograms. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images were taken from the Digital Database for Screening Mammography (DDSM), a database which consists of 2620 mammogram studies. These images are X-rays, recorded as grey-scale images. Previous work had been done to filter these images to the ROI mass. Within features_matrix.csv, results of calculations on these masses have been recorded. We have didvided the data into training and testing at random, leaving 10% out for testing. 52% of these images are negatively labeled while 48% are positively labeled. \n",
    "\n",
    "In addition, semantic features associated with some on the images have also been included. These semantic features include the type of view included in the image, which side of the patient the breast was on, and case descriptions with a limited vocabulary describing characteristics of the mass. We believe that semantic features such as breast density, mass shape, and mass margins can be highly useful. The limited vocabulary of these qualitative features allows one-hot encoding and incorporation into our feature vectors for traditional ML methods. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try to leverage the following features in order to make attempts at classification: \n",
    "* features in the features_matrix.csv (ASM, Area, Centroid coordinates, ...) \n",
    "* convexity, solidity, and extent. (Calculated from masked ROI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the packages that we're going to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/miniconda2/envs/py36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.morphology import convex_hull_image\n",
    "from skimage import data, img_as_float\n",
    "from skimage.util import invert\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.measure import regionprops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining global variables for paths, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_prop = .1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that calculates convex hull perimeter and area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_convexity_and_solidity(data): \n",
    "    ret,thresh_img = cv2.threshold(data,0,255,cv2.THRESH_BINARY)\n",
    "    thresh_props =  regionprops(thresh_img.astype(int))\n",
    "    thresh_area = thresh_props[0].area\n",
    "    thresh_perimeter = thresh_props[0].perimeter\n",
    "    chull = convex_hull_image(thresh_img)\n",
    "    props = regionprops(chull.astype(int))\n",
    "    chull_area = props[0].area\n",
    "    chull_perimeter = props[0].perimeter \n",
    "    convexity = chull_perimeter/thresh_perimeter\n",
    "    solidity = thresh_area/chull_area\n",
    "    return convexity, solidity, thresh_props[0].extent\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that reads in image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_images(data_parent_dir):\n",
    "    '''\n",
    "        input:\n",
    "            * paths, a list of the paths that we need to input \n",
    "        output:\n",
    "            * a dataframe containing image data, label, and name \n",
    "    '''\n",
    "    print(\"preparing to read images\")\n",
    "    \n",
    "    # initialize data structures \n",
    "    data = []\n",
    "    label = []\n",
    "    name = []\n",
    "    convexities = []\n",
    "    solidities = []\n",
    "    extents = []\n",
    "    \n",
    "    paths = os.listdir(data_parent_dir)\n",
    "\n",
    "    scan_ids = []\n",
    "    \n",
    "    # iteration over paths, dirs\n",
    "    for first_path in paths:\n",
    "        if first_path[0] == '.': # check for random dot files that come up :( \n",
    "            continue\n",
    "        local_dir = os.path.join(data_parent_dir, first_path)\n",
    "        for image in os.listdir(local_dir):\n",
    "            scan_ids.append(\"_\".join((first_path, image[:-3])))\n",
    "            with h5py.File(os.path.join(local_dir, image), 'r') as hf:\n",
    "                data.append(np.array(hf.get('data')))\n",
    "                label.append(np.array(hf.get('label')).item(0))\n",
    "                name.append(np.array(hf.get('name')))\n",
    "                convexity, solidity, extent = get_convexity_and_solidity(hf.get('data')[:, :, 1])\n",
    "                convexities.append(convexity)\n",
    "                solidities.append(solidity)\n",
    "                extents.append(extent)\n",
    "         \n",
    "    print(scan_ids[:10])\n",
    "    d = {'pixel_data':data, 'label':label, 'name':name, 'convexity': convexities, 'solidity': solidities, 'extent': extents}\n",
    "   \n",
    "    df = pd.DataFrame(data=d)\n",
    "\n",
    "    print(len(df[df['label'] == 0]))\n",
    "    print(len(df[df['label'] == 1]))\n",
    "    print(len(df))\n",
    "    \n",
    "    print(df.columns)\n",
    "    return (df, scan_ids)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_excess_rows(scan_ids, precomputed_df):\n",
    "\n",
    "    drop_list = []\n",
    "    true_list = []\n",
    "    scan_ids = set([\"P_\" +scan_id for scan_id in scan_ids])\n",
    "    names = precomputed_df.index.values\n",
    "    for name in names:\n",
    "        if name not in scan_ids:\n",
    "            drop_list.append(name)\n",
    "\n",
    "    print(\"the drop list has: \" + str(len(drop_list)))\n",
    "    return precomputed_df.drop(drop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially predict some potential issues with this approach: the number of features is large, and many are potentially correlated. For that reason, we can implement some dimensionality reduction and take a look at the variation explained by the principal components (and their contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(raw_data, new_dims=3):\n",
    "    '''\n",
    "        input:\n",
    "            * raw_data, the raw matrix that will be reduced in dimensionality \n",
    "        output:\n",
    "            * the dimensionality-reduced data \n",
    "        \n",
    "    '''\n",
    "    print(\"preparing to reduce dimensionality\")\n",
    "    pca = PCA()\n",
    "    pca.fit(raw_data)\n",
    "    print(\">>> variance explained by each principal component\")\n",
    "    print(pca.explained_variance_ratio_)  \n",
    "    print(\">>> the first principal component\")\n",
    "    print(pca.components_[0])\n",
    "    reduced = pca.transform(raw_data)[:,:new_dims]\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginnings of our preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_parent_dir, precomputed_path, dim_reduc=None):\n",
    "    print('Entering preprocessing')\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    # reading things in \n",
    "    image_df, scan_ids = gather_images(data_parent_dir)\n",
    "    precomputed_df = drop_excess_rows(scan_ids, pd.read_csv(precomputed_path, index_col=0))\n",
    "\n",
    "    print(image_df.describe())\n",
    "    print(precomputed_df.describe())\n",
    "    \n",
    "    # zero-centering, normalization\n",
    "    precomputed_fts = precomputed_df.values[:,1:]\n",
    "    print(precomputed_df.values[:,1:])\n",
    "    print(precomputed_fts.shape)\n",
    "    new_fts = np.array(precomputed_fts,dtype=np.float32)   \n",
    "    new_fts -= np.mean(new_fts, axis = 0)\n",
    "    new_fts /= np.std(new_fts, axis=0)\n",
    "    print(\"==============================\")\n",
    "    print(new_fts.shape)\n",
    "    \n",
    "    # dimensionality reduction \n",
    "    if dim_reduc is not None:\n",
    "        new_fts = reduce_dimensionality(new_fts, dim_reduc)\n",
    "        feature_names += [\"pc# \"+str(x) for x in range(1, dim_reduc+1)]\n",
    "        precomputed_df = pd.DataFrame(new_fts, columns=feature_names, index=precomputed_df.index)\n",
    "        print(\"==============================\")\n",
    "        print(precomputed_df.info())\n",
    "        \n",
    "    # adding back to df \n",
    "    else:\n",
    "        feature_names += list(precomputed_df)[1:]\n",
    "        precomputed_df[feature_names] = new_fts\n",
    "        print(\"==============================\")\n",
    "        print(precomputed_df.info())\n",
    "    \n",
    "    feature_names += [\"convexity\", \"solidity\", \"extent\"]\n",
    "    \n",
    "    # joining dfs\n",
    "    df = image_df.join(precomputed_df)\n",
    "\n",
    "    df.info()\n",
    " #   feature_names.append(\"pixel_data\")\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    \n",
    "    return (train[feature_names], train['label'], test[feature_names], test['label'], scan_ids)\n",
    "#    return train, test, feature_names\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've pretty much established how our data should be preprocessed, we want to introduce and train some basic machine learning models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train, labels):\n",
    "    \n",
    "    #lassoClf = Lasso()\n",
    "    #lassoClf.fit(train, labels)\n",
    "    \n",
    "    svcClf = SVC(random_state = 260)\n",
    "    svcClf.fit(train, labels)\n",
    "    \n",
    "    rfClf = RandomForestClassifier(random_state = 260)\n",
    "    rfClf.fit(train, labels)\n",
    "    \n",
    "    return [svcClf, rfClf]\n",
    "\n",
    "\n",
    "  #  return [lassoClf, svcClf, rfClf]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_models(models, test, y_true):\n",
    "    '''\n",
    "        input: \n",
    "            * models, a list of trained models \n",
    "            * test, the test set on which models will be evaluated\n",
    "            * feature_names, the list of feature columns that are being used \n",
    "    '''\n",
    "    print(\"Preparing to validate models\")\n",
    "    for model in models:\n",
    "        y_pred = model.predict(test)\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        #print(y_pred)\n",
    "        print(\"auroc: \" +  str(roc_auc_score(y_true, y_pred)))\n",
    "        \n",
    "    print(\"Done validating models\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    data_parent_dir = \"./data_fixed_crop_w_mask\"\n",
    "    precomputed_path = \"features_matrix.csv\"\n",
    "    X_train, y_train, X_test, y_test, all_IDs = preprocess(data_parent_dir, precomputed_path, dim_reduc=None)\n",
    "    models = train_models(X_train, y_train)\n",
    "    validate_models(models, X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering preprocessing\n",
      "preparing to read images\n",
      "['00001_LEFT_CC', '00001_LEFT_MLO', '00004_LEFT_CC', '00004_LEFT_MLO', '00004_RIGHT_CC', '00004_RIGHT_MLO', '00009_RIGHT_CC', '00009_RIGHT_MLO', '00015_LEFT_MLO', '00016_LEFT_CC']\n",
      "786\n",
      "722\n",
      "1508\n",
      "Index(['pixel_data', 'label', 'name', 'convexity', 'solidity', 'extent'], dtype='object')\n",
      "the drop list has: 91\n",
      "             label    convexity     solidity       extent\n",
      "count  1508.000000  1508.000000  1508.000000  1508.000000\n",
      "mean      0.478780     0.823225     0.863078     0.644197\n",
      "std       0.499715     0.151674     0.117002     0.107757\n",
      "min       0.000000     0.461828     0.047736     0.016178\n",
      "25%       0.000000     0.754172     0.853156     0.620854\n",
      "50%       0.000000     0.821002     0.885096     0.662134\n",
      "75%       1.000000     0.876884     0.912385     0.699197\n",
      "max       1.000000     3.014770     0.985606     0.873839\n",
      "               ASM           Area   Centroid_x   Centroid_y     Contrast  \\\n",
      "count  1508.000000    1508.000000  1508.000000  1508.000000  1508.000000   \n",
      "mean      0.989816   82196.687666     0.509939     0.516854     0.002132   \n",
      "std       0.009185   72112.671733     0.155333     0.245478     0.001284   \n",
      "min       0.905100    4567.000000     0.093025     0.019573     0.000200   \n",
      "25%       0.987500   38052.750000     0.392484     0.325622     0.001200   \n",
      "50%       0.992500   59975.000000     0.509351     0.522706     0.001900   \n",
      "75%       0.995700  100590.500000     0.624067     0.722545     0.002600   \n",
      "max       0.999200  656650.000000     0.923238     0.986207     0.009100   \n",
      "\n",
      "       Dissimilarity  Eccentricity       Energy  Homogeneity  Max intensity  \\\n",
      "count    1508.000000   1508.000000  1508.000000  1508.000000    1508.000000   \n",
      "mean        0.000978      0.569280     0.994883     0.999626     188.931134   \n",
      "std         0.000613      0.165532     0.004645     0.000258      31.375588   \n",
      "min         0.000100      0.149002     0.951300     0.997500      56.377432   \n",
      "25%         0.000600      0.459396     0.993700     0.999500     171.147860   \n",
      "50%         0.000800      0.570317     0.996300     0.999700     191.642023   \n",
      "75%         0.001200      0.671370     0.997800     0.999800     210.673617   \n",
      "max         0.005100      0.999369     0.999600     1.000000     255.000000   \n",
      "\n",
      "       Mean intensity  Min intensity  Orientation    Perimeter  \\\n",
      "count     1508.000000    1508.000000  1508.000000  1508.000000   \n",
      "mean       150.672146     102.798374     0.025581  1880.514226   \n",
      "std         32.871146      40.443288     0.890460   983.778491   \n",
      "min         29.622591       0.000000    -1.570040   370.143182   \n",
      "25%        131.409523      80.486381    -0.618421  1218.628859   \n",
      "50%        153.473748     108.038911     0.020111  1622.937300   \n",
      "75%        173.558266     131.247082     0.678797  2209.163743   \n",
      "max        231.298409     203.706897     1.570796  8591.265146   \n",
      "\n",
      "       Relative intensity     Solidity  StdDev intensity  Texture correlation  \n",
      "count         1508.000000  1508.000000       1508.000000          1508.000000  \n",
      "mean             0.740981     0.834554         10.139930             0.962781  \n",
      "std              0.123214     0.118032          5.354910             0.013902  \n",
      "min              0.221088     0.038955          0.982200             0.875400  \n",
      "25%              0.671168     0.817636          6.313000             0.955500  \n",
      "50%              0.751537     0.855264          9.042100             0.964800  \n",
      "75%              0.826171     0.886733         12.763475             0.972300  \n",
      "max              1.063823     0.989777         35.282200             0.994700  \n",
      "[[1.20561000e+05 5.49229572e-01 1.46999451e-01 ... 8.27080203e-01\n",
      "  1.29682000e+01 9.71200000e-01]\n",
      " [3.21980000e+04 6.45280607e-01 1.41495005e-01 ... 8.19976061e-01\n",
      "  8.49650000e+00 9.53900000e-01]\n",
      " [1.13289000e+05 6.75677025e-01 5.40257966e-01 ... 8.53195463e-01\n",
      "  1.34881000e+01 9.68300000e-01]\n",
      " ...\n",
      " [2.12345000e+05 3.24883809e-01 1.20111913e-01 ... 8.99896595e-01\n",
      "  2.27938000e+01 9.83000000e-01]\n",
      " [2.31530000e+04 3.65885117e-01 1.26335115e-01 ... 8.72940467e-01\n",
      "  7.55820000e+00 9.45800000e-01]\n",
      " [3.48530000e+04 6.76333626e-01 4.07978412e-02 ... 7.25408983e-01\n",
      "  9.53980000e+00 9.47000000e-01]]\n",
      "(1508, 17)\n",
      "==============================\n",
      "(1508, 17)\n",
      "==============================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1508 entries, P_00001_LEFT_CC to P_01557_RIGHT_MLO\n",
      "Data columns (total 18 columns):\n",
      "ASM                    1508 non-null float64\n",
      "Area                   1508 non-null float64\n",
      "Centroid_x             1508 non-null float64\n",
      "Centroid_y             1508 non-null float64\n",
      "Contrast               1508 non-null float64\n",
      "Dissimilarity          1508 non-null float64\n",
      "Eccentricity           1508 non-null float64\n",
      "Energy                 1508 non-null float64\n",
      "Homogeneity            1508 non-null float64\n",
      "Max intensity          1508 non-null float64\n",
      "Mean intensity         1508 non-null float64\n",
      "Min intensity          1508 non-null float64\n",
      "Orientation            1508 non-null float64\n",
      "Perimeter              1508 non-null float64\n",
      "Relative intensity     1508 non-null float64\n",
      "Solidity               1508 non-null float64\n",
      "StdDev intensity       1508 non-null float64\n",
      "Texture correlation    1508 non-null float64\n",
      "dtypes: float64(18)\n",
      "memory usage: 223.8+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1508 entries, 0 to 1507\n",
      "Data columns (total 24 columns):\n",
      "pixel_data             1508 non-null object\n",
      "label                  1508 non-null int64\n",
      "name                   1508 non-null object\n",
      "convexity              1508 non-null float64\n",
      "solidity               1508 non-null float64\n",
      "extent                 1508 non-null float64\n",
      "ASM                    0 non-null float64\n",
      "Area                   0 non-null float64\n",
      "Centroid_x             0 non-null float64\n",
      "Centroid_y             0 non-null float64\n",
      "Contrast               0 non-null float64\n",
      "Dissimilarity          0 non-null float64\n",
      "Eccentricity           0 non-null float64\n",
      "Energy                 0 non-null float64\n",
      "Homogeneity            0 non-null float64\n",
      "Max intensity          0 non-null float64\n",
      "Mean intensity         0 non-null float64\n",
      "Min intensity          0 non-null float64\n",
      "Orientation            0 non-null float64\n",
      "Perimeter              0 non-null float64\n",
      "Relative intensity     0 non-null float64\n",
      "Solidity               0 non-null float64\n",
      "StdDev intensity       0 non-null float64\n",
      "Texture correlation    0 non-null float64\n",
      "dtypes: float64(21), int64(1), object(2)\n",
      "memory usage: 334.5+ KB\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-29bc503c7850>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-e15a5416ab67>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprecomputed_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"features_matrix.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_IDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_parent_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecomputed_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_reduc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mvalidate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-3db7ad794d61>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(train, labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msvcClf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m260\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msvcClf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrfClf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m260\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now that we've seen our models perform like garbage, we're going to try to add in the semantic feature encoding to see if that will help "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gather_semantic_features(semantics_path):\n",
    "    semantics_path = \"mass_case_description_train_set.csv\"\n",
    "    semantic_df = pd.read_csv(semantics_path)\n",
    "    semantic_df.dropna(inplace=True)\n",
    "    return semantic_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_categorical_labels(semantic_df, semantic_feature_names):\n",
    "    '''\n",
    "        input:\n",
    "        \n",
    "        output:\n",
    "        \n",
    "    '''\n",
    "    for feature in semantic_feature_names:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        #print(semantic_df[feature].values)\n",
    "        #print(semantic_df[semantic_df[feature].isnull() == True])\n",
    "        le.fit(list(semantic_df[feature].astype(str)))\n",
    "        print(le.classes_)\n",
    "        semantic_df[feature] = le.transform(semantic_df[feature])\n",
    "    return semantic_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(semantic_df, semantic_feature_names):\n",
    "    '''\n",
    "        input:\n",
    "        \n",
    "        output:\n",
    "        \n",
    "    '''\n",
    "    enc = preprocessing.OneHotEncoder(sparse=False)\n",
    "    semantic_one_hots = enc.fit_transform(semantic_df[semantic_feature_names])\n",
    "    print(semantic_one_hots)\n",
    "    _, one_hot_length = semantic_one_hots.shape\n",
    "    return (semantic_one_hots, one_hot_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_semantic_df(semantics_path, semantic_feature_names, total_patientIDs):\n",
    "    \n",
    "    semantic_df = gather_semantic_features(semantics_path)\n",
    "    semantic_df = encode_categorical_labels(semantic_df, semantic_feature_names)\n",
    "    semantic_one_hots, one_hot_length = one_hot_encoding(semantic_df, semantic_feature_names)\n",
    "   \n",
    "    has_semantic = [s1 + \"_\" + s2 + \"_\" + s3 for (s1, s2, s3) in zip(list(semantic_df['patient_id']), list(semantic_df['side']), list(semantic_df['view']))]\n",
    "\n",
    "    semantic_encoded_dict = {img:np.zeros(one_hot_length) for (idx, img) in enumerate(total_patientIDs)}\n",
    "\n",
    "    counter = 0\n",
    "    for img, patient_id in enumerate(has_semantic): \n",
    "        if patient_id in semantic_encoded_dict.keys():\n",
    "            semantic_encoded_dict[patient_id] = semantic_one_hots[img] \n",
    "        else: \n",
    "            counter += 1\n",
    "        \n",
    "    print(one_hot_length)\n",
    "    print(counter)\n",
    "    print(len(total_patientIDs))\n",
    "    \n",
    "    encoded_feature_names = [\"one_hot #\" + str(x) for x in range(1, one_hot_length+1)]\n",
    "    semantic_encoded_df = pd.DataFrame.from_dict(semantic_encoded_dict, orient='index', columns=encoded_feature_names)\n",
    "    print(semantic_encoded_df.describe())\n",
    "    return (semantic_encoded_df, encoded_feature_names)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we find that there are semantic descriptions of images that do not appear in the h5'd dataset, and also images that appear in this dataset without corresponding semantic descriptions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess2(data_parent_dir, precomputed_path, semantics_path, semantic_feature_names, dim_reduc=None):\n",
    "    print('Entering preprocessing')\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    # reading things in \n",
    "    image_df = gather_images(data_parent_dir)\n",
    "    precomputed_df = pd.read_csv(precomputed_path)\n",
    "\n",
    "    # zero-centering, normalization\n",
    "    precomputed_fts = precomputed_df.values[:,1:]\n",
    "    new_fts = np.array(precomputed_fts,dtype=np.float32)   \n",
    "    new_fts -= np.mean(new_fts, axis = 0)\n",
    "    new_fts /= np.std(new_fts, axis=0)\n",
    "    \n",
    "    # dimensionality reduction \n",
    "    if dim_reduc is not None:\n",
    "        new_fts = reduce_dimensionality(new_fts, dim_reduc)\n",
    "        feature_names += [\"pc# \"+str(x) for x in range(1, dim_reduc+1)]\n",
    "        precomputed_df = pd.DataFrame(new_fts, columns=feature_names, index=precomputed_df.index)\n",
    "        \n",
    "    # adding back to df \n",
    "    else:\n",
    "        feature_names += list(precomputed_df)[1:]\n",
    "        precomputed_df[feature_names] = new_fts\n",
    "    \n",
    "    # get the list of all images\n",
    "    total_patientIDs = precomputed_df.values[:,0]\n",
    "    \n",
    "    # get the semantic df\n",
    "    semantic_df, encoded_feature_names = generate_semantic_df(semantics_path, semantic_feature_names, total_patientIDs)\n",
    "    feature_names += encoded_feature_names\n",
    "    \n",
    "    # joining dfs\n",
    "    df = image_df.join(precomputed_df)\n",
    "    df = df.join(semantic_df)\n",
    "\n",
    "    print(df.describe())\n",
    " #   feature_names.append(\"pixel_data\")\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    \n",
    "    return (train[feature_names], train['label'], test[feature_names], test['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "semantic_label_name = ['pathology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipeline2():\n",
    "    data_parent_dir = \"./data_fixed_crop_w_mask\"\n",
    "    precomputed_path = \"features_matrix.csv\"\n",
    "    semantic_feature_names = ['breast_density', 'abn_num', 'mass_shape', 'mass_margins', 'assessment']\n",
    "    X_train, y_train, X_test, y_test = preprocess2(data_parent_dir, \n",
    "                                                           precomputed_path, \n",
    "                                                           semantics_path, \n",
    "                                                           semantic_feature_names, \n",
    "                                                           dim_reduc=3)\n",
    "    models = train_models(X_train, y_train)\n",
    "    validate_models(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering preprocessing\n",
      "preparing to read images\n",
      "786\n",
      "722\n",
      "1508\n",
      "preparing to reduce dimensionality\n",
      ">>> variance explained by each principal component\n",
      "[4.4473657e-01 1.6824159e-01 9.1169290e-02 5.7367690e-02 5.6748547e-02\n",
      " 5.2320685e-02 3.6198188e-02 3.1822890e-02 2.2656877e-02 1.7601196e-02\n",
      " 9.3034180e-03 7.1804966e-03 2.3674807e-03 1.1376145e-03 8.7159796e-04\n",
      " 2.6753976e-04 6.5508975e-06 1.8152512e-06]\n",
      ">>> the first principal component\n",
      "[-0.33582258  0.3129383  -0.04206447 -0.01204051  0.31197217  0.3208049\n",
      "  0.00518964 -0.33550972 -0.30273733  0.21556541  0.16609216  0.02282244\n",
      "  0.00318173  0.30442756  0.21560022 -0.05786494  0.3394732   0.22484674]\n",
      "['1' '2' '3' '4']\n",
      "['1' '2' '3' '4' '5' '6']\n",
      "['ARCHITECTURAL_DISTORTION' 'ASYMMETRIC_BREAST_TISSUE'\n",
      " 'FOCAL_ASYMMETRIC_DENSITY' 'IRREGULAR'\n",
      " 'IRREGULAR-ARCHITECTURAL_DISTORTION' 'IRREGULAR-FOCAL_ASYMMETRIC_DENSITY'\n",
      " 'LOBULATED' 'LOBULATED-ARCHITECTURAL_DISTORTION' 'LOBULATED-IRREGULAR'\n",
      " 'LOBULATED-LYMPH_NODE' 'LOBULATED-OVAL' 'LYMPH_NODE' 'OVAL'\n",
      " 'OVAL-LYMPH_NODE' 'ROUND' 'ROUND-IRREGULAR-ARCHITECTURAL_DISTORTION'\n",
      " 'ROUND-LOBULATED' 'ROUND-OVAL']\n",
      "['CIRCUMSCRIBED' 'CIRCUMSCRIBED-ILL_DEFINED'\n",
      " 'CIRCUMSCRIBED-MICROLOBULATED' 'CIRCUMSCRIBED-OBSCURED' 'ILL_DEFINED'\n",
      " 'ILL_DEFINED-SPICULATED' 'MICROLOBULATED' 'MICROLOBULATED-ILL_DEFINED'\n",
      " 'MICROLOBULATED-ILL_DEFINED-SPICULATED' 'MICROLOBULATED-SPICULATED'\n",
      " 'OBSCURED' 'OBSCURED-ILL_DEFINED' 'OBSCURED-ILL_DEFINED-SPICULATED'\n",
      " 'OBSCURED-SPICULATED' 'SPICULATED']\n",
      "['0' '1' '2' '3' '4' '5']\n",
      "[[0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]]\n",
      "49\n",
      "1273\n",
      "1599\n",
      "       one_hot #1  one_hot #2  one_hot #3  one_hot #4  one_hot #5  one_hot #6  \\\n",
      "count      1599.0      1599.0      1599.0      1599.0      1599.0      1599.0   \n",
      "mean          0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "std           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "min           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "25%           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "50%           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "75%           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "max           0.0         0.0         0.0         0.0         0.0         0.0   \n",
      "\n",
      "       one_hot #7  one_hot #8  one_hot #9  one_hot #10     ...       \\\n",
      "count      1599.0      1599.0      1599.0       1599.0     ...        \n",
      "mean          0.0         0.0         0.0          0.0     ...        \n",
      "std           0.0         0.0         0.0          0.0     ...        \n",
      "min           0.0         0.0         0.0          0.0     ...        \n",
      "25%           0.0         0.0         0.0          0.0     ...        \n",
      "50%           0.0         0.0         0.0          0.0     ...        \n",
      "75%           0.0         0.0         0.0          0.0     ...        \n",
      "max           0.0         0.0         0.0          0.0     ...        \n",
      "\n",
      "       one_hot #40  one_hot #41  one_hot #42  one_hot #43  one_hot #44  \\\n",
      "count       1599.0       1599.0       1599.0       1599.0       1599.0   \n",
      "mean           0.0          0.0          0.0          0.0          0.0   \n",
      "std            0.0          0.0          0.0          0.0          0.0   \n",
      "min            0.0          0.0          0.0          0.0          0.0   \n",
      "25%            0.0          0.0          0.0          0.0          0.0   \n",
      "50%            0.0          0.0          0.0          0.0          0.0   \n",
      "75%            0.0          0.0          0.0          0.0          0.0   \n",
      "max            0.0          0.0          0.0          0.0          0.0   \n",
      "\n",
      "       one_hot #45  one_hot #46  one_hot #47  one_hot #48  one_hot #49  \n",
      "count       1599.0       1599.0       1599.0       1599.0       1599.0  \n",
      "mean           0.0          0.0          0.0          0.0          0.0  \n",
      "std            0.0          0.0          0.0          0.0          0.0  \n",
      "min            0.0          0.0          0.0          0.0          0.0  \n",
      "25%            0.0          0.0          0.0          0.0          0.0  \n",
      "50%            0.0          0.0          0.0          0.0          0.0  \n",
      "75%            0.0          0.0          0.0          0.0          0.0  \n",
      "max            0.0          0.0          0.0          0.0          0.0  \n",
      "\n",
      "[8 rows x 49 columns]\n",
      "             label        pc# 1        pc# 2        pc# 3  one_hot #1  \\\n",
      "count  1508.000000  1508.000000  1508.000000  1508.000000         0.0   \n",
      "mean      0.478780    -0.020880    -0.003538     0.006279         NaN   \n",
      "std       0.499715     2.824638     1.738863     1.296816         NaN   \n",
      "min       0.000000    -5.887184    -4.303214    -3.578173         NaN   \n",
      "25%       0.000000    -1.916224    -1.176806    -0.759557         NaN   \n",
      "50%       0.000000    -0.533474    -0.170123    -0.128470         NaN   \n",
      "75%       1.000000     1.197309     0.971187     0.506527         NaN   \n",
      "max       1.000000    18.382227     5.955268     7.663785         NaN   \n",
      "\n",
      "       one_hot #2  one_hot #3  one_hot #4  one_hot #5  one_hot #6  \\\n",
      "count         0.0         0.0         0.0         0.0         0.0   \n",
      "mean          NaN         NaN         NaN         NaN         NaN   \n",
      "std           NaN         NaN         NaN         NaN         NaN   \n",
      "min           NaN         NaN         NaN         NaN         NaN   \n",
      "25%           NaN         NaN         NaN         NaN         NaN   \n",
      "50%           NaN         NaN         NaN         NaN         NaN   \n",
      "75%           NaN         NaN         NaN         NaN         NaN   \n",
      "max           NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "          ...       one_hot #40  one_hot #41  one_hot #42  one_hot #43  \\\n",
      "count     ...               0.0          0.0          0.0          0.0   \n",
      "mean      ...               NaN          NaN          NaN          NaN   \n",
      "std       ...               NaN          NaN          NaN          NaN   \n",
      "min       ...               NaN          NaN          NaN          NaN   \n",
      "25%       ...               NaN          NaN          NaN          NaN   \n",
      "50%       ...               NaN          NaN          NaN          NaN   \n",
      "75%       ...               NaN          NaN          NaN          NaN   \n",
      "max       ...               NaN          NaN          NaN          NaN   \n",
      "\n",
      "       one_hot #44  one_hot #45  one_hot #46  one_hot #47  one_hot #48  \\\n",
      "count          0.0          0.0          0.0          0.0          0.0   \n",
      "mean           NaN          NaN          NaN          NaN          NaN   \n",
      "std            NaN          NaN          NaN          NaN          NaN   \n",
      "min            NaN          NaN          NaN          NaN          NaN   \n",
      "25%            NaN          NaN          NaN          NaN          NaN   \n",
      "50%            NaN          NaN          NaN          NaN          NaN   \n",
      "75%            NaN          NaN          NaN          NaN          NaN   \n",
      "max            NaN          NaN          NaN          NaN          NaN   \n",
      "\n",
      "       one_hot #49  \n",
      "count          0.0  \n",
      "mean           NaN  \n",
      "std            NaN  \n",
      "min            NaN  \n",
      "25%            NaN  \n",
      "50%            NaN  \n",
      "75%            NaN  \n",
      "max            NaN  \n",
      "\n",
      "[8 rows x 53 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-53afb9f04d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-73745680611b>\u001b[0m in \u001b[0;36mpipeline2\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                            \u001b[0msemantic_feature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                            dim_reduc=3)\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mvalidate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-3db7ad794d61>\u001b[0m in \u001b[0;36mtrain_models\u001b[0;34m(train, labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msvcClf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m260\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msvcClf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrfClf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m260\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/svm/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/envs/py36/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "pipeline2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "Bradski, G. (2000). The OpenCV Library. Dr. Dobb's Journal of Software Tools\n",
    "\n",
    "Britton P, Warwick J, Wallis MG, et al. Measuring the accuracy of diagnostic imaging in symptomatic breast patients: team and individual performance. The British Journal of Radiology. 2012;85(1012):415-422. doi:10.1259/bjr/32906819.\n",
    "\n",
    "Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
    "\n",
    "StÃ©fan van der Walt, Johannes L. SchÃ¶nberger, Juan Nunez-Iglesias, FranÃ§ois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu and the scikit-image contributors. scikit-image: Image processing in Python. PeerJ 2:e453 (2014) http://dx.doi.org/10.7717/peerj.453\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
