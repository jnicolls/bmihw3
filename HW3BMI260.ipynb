{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMI 260 ASSIGNMENT #3 | Mammogram Spring 2018\n",
    "\n",
    "## Name 1:Joseph Nicolls\n",
    "\n",
    "## Name 2: Alex Lu\n",
    "\n",
    "Breast cancer has the highest incidence and second highest mortality rate for women in the US. \n",
    "Your task is to utilize machine learning to either classify AND/OR segment mammograms or neither, as long as you justify why it is useful to do whatever it is you want to do. If someone turns in a deep dream assignment using mammograms this might be amusing, but not so useful to patients. Consider this a mini-project, we highly suggest you work with 1 other person, it can be someone in your team. \n",
    "\n",
    "In addition to the mammograms, the dataset includes segmentations and mass_case_description_train_set.csv, which contains information about the mass shape, mass margins, assessment number, pathology diagnosis, and subtlety. Take some time to research what all of these different fields mean and whether you can use them in your algorithm or not. You dont need to use all of what is provided to you. \n",
    "\n",
    "Some ideas:\n",
    "\n",
    "1. use the ROIâ€™s or segmentations to extract features, and then train a classifier based on those features using the algorithms presented to you in the machine learning lectures, does not need to be deep learning. \n",
    "\n",
    "2. use convolutional neural networks, feel free to use any of the code we went over in class or use your own (custom code, sklearn, keras, Tensorflow etc.). If you dont want to place helper functions and classes into this notebook, place them in a .py file in the same folder called helperfunctions.py and import them into this notebook. \n",
    "\n",
    "The data is here:\n",
    "\n",
    "https://wiki.cancerimagingarchive.net/display/Public/CBIS-DDSM\n",
    "\n",
    "If you do not like python, you can use a different language and turn your assignment in as a folder with all your code, a folder with all your figures and a latex or doc file with the writeup. The writeup doesnt need to be long, 1 page will do and cite at least one clinical paper and one technical paper. If you like python please place the writeup and code into this notebook. Use the markdown to tell is what you are doing in each section. You will not be graded on the performance of your model, only the scientific soundness of your claims, methodology, evaluation (ie fair but insightful statistics) and discussion of the shortcomings of what you tried. \n",
    "\n",
    "The format for this homework is as follows:\n",
    "\n",
    "1. Describe what you are doing and why it matters to patients using at least one citation\n",
    "\n",
    "2. Describe the relevant statistics of the data, how were the images taken ? how were they labeled ? what is the class balance and majority classifier accuracy ? How will you divide the data into testing, training and validation.\n",
    "\n",
    "3. Describe your data pipeline, ie how is the data scrubbed, normalized, stored and fed to the model for training. \n",
    "\n",
    "4. Explain how the model you chose works along side the code for it and at least one technical citation to give credit where credit is due. \n",
    "\n",
    "5. There are many ways to do training, take us through how you do it. (ie we used early stopping and we decided when to stop based on validation loss)  \n",
    "\n",
    "6. Make a figure displaying your results\n",
    "\n",
    "7. Discuss pros and cons of your method and what you might have done differently now that youve tried or would try if you had more time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional ML techniques for Classification of Mammograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Approach and Relevance\n",
    "\n",
    "Mammograms are difficult to classify because identifying features for malignant tumors can be vague and masses in mammograms can appear anywhere with any orientation in the breast tissue. Though the setup for this problem suggests use of CNNs, we want to use traditional ML techniques in this exploratory research for a variety of reasons. The principal reason that we want to do this is for the feature analysis possible with traditional ML technqiues that isn't possible with CNNs. According to a study from Britton et. all, sensitivity of radiologists in classifying mammograms can vary between 53.1-74.1%. Through feature analysis, indiciative features could be highlighted for radiologists to focus on, potentially raising sensitivites and decreasing the variability of senestivities between radiologists. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images were taken from the Digital Database for Screening Mammography (DDSM), a database which consists of 2620 mammogram studies. These images are X-rays, recorded as grey-scale images. Previous work had been done to filter these images to the ROI mass. Within features_matrix.csv, results of calculations on these masses have been recorded. We have didvided the data into training and testing at random, leaving 10% out for testing. 52% of these images are negatively labeled while 48% are positively labeled. \n",
    "\n",
    "In addition, semantic features associated with some on the images have also been included. These semantic features include the type of view included in the image, which side of the patient the breast was on, and case descriptions with a limited vocabulary describing characteristics of the mass. We believe that semantic features such as breast density, mass shape, and mass margins can be highly useful. The limited vocabulary of these qualitative features allows one-hot encoding and incorporation into our feature vectors for traditional ML methods. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first try to leverage the following features in order to make attempts at classification: \n",
    "* features in the features_matrix.csv (ASM, Area, Centroid coordinates, ...) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import the packages that we're going to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Britton P, Warwick J, Wallis MG, et al. Measuring the accuracy of diagnostic imaging in symptomatic breast patients: team and individual performance. The British Journal of Radiology. 2012;85(1012):415-422. doi:10.1259/bjr/32906819."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining global variables for paths, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_parent_dir = \"./data_fixed_crop_w_mask\"\n",
    "precomputed_path = \"features_matrix.csv\"\n",
    "validation_prop = .1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that reads in image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_features(paths):\n",
    "    '''\n",
    "        input:\n",
    "            * paths, a list of the paths that we need to input \n",
    "        output:\n",
    "            * a dataframe containing image data, label, and name \n",
    "    '''\n",
    "    print(\"preparing to read images\")\n",
    "    \n",
    "    # initialize data structures \n",
    "    data = []\n",
    "    label = []\n",
    "    name = []\n",
    "    \n",
    "    # iteratio nover paths, dirs\n",
    "    for first_path in paths:\n",
    "        if first_path[0] == '.': # check for random dot files that come up :( )\n",
    "            continue\n",
    "        local_dir = os.path.join(data_parent_dir, first_path)\n",
    "        for image in os.listdir(local_dir):\n",
    "            hf = h5py.File(os.path.join(local_dir, image), 'r')\n",
    "            data.append(np.array(hf.get('data')))\n",
    "            label.append(np.array(hf.get('label')).item(0))\n",
    "            name.append(np.array(hf.get('name')))\n",
    "    d = {'pixel_data':data, 'label':label, 'name':name}\n",
    "   \n",
    "    df = pd.DataFrame(data=d)\n",
    "    print(len(df[df['label'] == 0]))\n",
    "    print(len(df[df['label'] == 1]))\n",
    "    print(len(df))\n",
    "    return df\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially predict some potential issues with this approach: the number of features is large, and many are potentially correlated. For that reason, we can implement some dimensionality reduction and take a look at the variation explained by the principal components (and their contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(raw_data, new_dims=3):\n",
    "    '''\n",
    "        input:\n",
    "            * raw_data, the raw matrix that will be reduced in dimensionality \n",
    "        output:\n",
    "            * the dimensionality-reduced data \n",
    "        \n",
    "    '''\n",
    "    print(\"preparing to reduce dimensionality\")\n",
    "    pca = PCA()\n",
    "    pca.fit(raw_data)\n",
    "    print(\">>> variance explained by each principal component\")\n",
    "    print(pca.explained_variance_ratio_)  \n",
    "    print(\">>> the first principal component\")\n",
    "    print(pca.components_[0])\n",
    "    reduced = pca.transform(raw_data)[:,:new_dims]\n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginnings of our preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dim_reduc=None):\n",
    "    print('Entering preprocessing')\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    # reading things in \n",
    "    all_paths = os.listdir(data_parent_dir)\n",
    "    image_df = get_image_features(all_paths)\n",
    "    precomputed_df = pd.read_csv(precomputed_path)\n",
    "\n",
    "    # zero-centering, normalization\n",
    "    precomputed_fts = precomputed_df.values[:,1:]\n",
    "    new_fts = np.array(precomputed_fts,dtype=np.float32)   \n",
    "    new_fts -= np.mean(new_fts, axis = 0)\n",
    "    new_fts /= np.std(new_fts, axis=0)\n",
    "    \n",
    "    total_patientIDs = precomputed_df.values[:,0]\n",
    "    # dimensionality reduction \n",
    "    if dim_reduc is not None:\n",
    "        new_fts = reduce_dimensionality(new_fts, dim_reduc)\n",
    "        feature_names += [\"pc# \"+str(x) for x in range(1, dim_reduc+1)]\n",
    "        precomputed_df = pd.DataFrame(new_fts, columns=feature_names, index=precomputed_df.index)\n",
    "        \n",
    "    # adding back to df \n",
    "    else:\n",
    "        feature_names += list(precomputed_df)[1:]\n",
    "        precomputed_df[feature_names] = new_fts\n",
    "    \n",
    "    # joining dfs\n",
    "    df = image_df.join(precomputed_df)\n",
    "\n",
    " #   feature_names.append(\"pixel_data\")\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    \n",
    "    return (train[feature_names], train['label'], test[feature_names], test['label'], total_patientIDs)\n",
    "#    return train, test, feature_names\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've pretty much established how our data should be preprocessed, we want to introduce and train some basic machine learning models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(train, labels):\n",
    "    \n",
    "    #lassoClf = Lasso()\n",
    "    #lassoClf.fit(train, labels)\n",
    "    \n",
    "    svcClf = SVC(random_state = 260)\n",
    "    svcClf.fit(train, labels)\n",
    "    \n",
    "    rfClf = RandomForestClassifier(random_state = 260)\n",
    "    rfClf.fit(train, labels)\n",
    "    \n",
    "    return [svcClf, rfClf]\n",
    "\n",
    "\n",
    "  #  return [lassoClf, svcClf, rfClf]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_models(models, test, y_true):\n",
    "    '''\n",
    "        input: \n",
    "            * models, a list of trained models \n",
    "            * test, the test set on which models will be evaluated\n",
    "            * feature_names, the list of feature columns that are being used \n",
    "    '''\n",
    "    print(\"Preparing to validate models\")\n",
    "    for model in models:\n",
    "        y_pred = model.predict(test)\n",
    "        print(confusion_matrix(y_true, y_pred))\n",
    "        #print(y_pred)\n",
    "        print(\"auroc: \" +  str(roc_auc_score(y_true, y_pred)))\n",
    "        \n",
    "    print(\"Done validating models\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline():\n",
    "    X_train, y_train, X_test, y_test, all_IDs = preprocess(dim_reduc=3)\n",
    "    models = train_models(X_train, y_train)\n",
    "    validate_models(models, X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering preprocessing\n",
      "preparing to read images\n",
      "786\n",
      "722\n",
      "1508\n",
      "preparing to reduce dimensionality\n",
      ">>> variance explained by each principal component\n",
      "[4.4473657e-01 1.6824159e-01 9.1169290e-02 5.7367690e-02 5.6748547e-02\n",
      " 5.2320685e-02 3.6198188e-02 3.1822890e-02 2.2656877e-02 1.7601196e-02\n",
      " 9.3034180e-03 7.1804966e-03 2.3674807e-03 1.1376145e-03 8.7159796e-04\n",
      " 2.6753976e-04 6.5508975e-06 1.8152512e-06]\n",
      ">>> the first principal component\n",
      "[-0.33582258  0.3129383  -0.04206447 -0.01204051  0.31197217  0.3208049\n",
      "  0.00518964 -0.33550972 -0.30273733  0.21556541  0.16609216  0.02282244\n",
      "  0.00318173  0.30442756  0.21560022 -0.05786494  0.3394732   0.22484674]\n",
      "Preparing to validate models\n",
      "[[92 66]\n",
      " [83 61]]\n",
      "auroc: 0.5029447960618847\n",
      "[[106  52]\n",
      " [ 88  56]]\n",
      "auroc: 0.529887482419128\n",
      "Done validating models\n"
     ]
    }
   ],
   "source": [
    "pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so now that we've seen our models perform like garbage, we're going to try to add in the semantic feature encoding to see if that will help "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1273 entries, 0 to 1317\n",
      "Data columns (total 13 columns):\n",
      "patient_id        1273 non-null object\n",
      "breast_density    1273 non-null int64\n",
      "side              1273 non-null object\n",
      "view              1273 non-null object\n",
      "abn_num           1273 non-null int64\n",
      "mass_shape        1273 non-null object\n",
      "mass_margins      1273 non-null object\n",
      "assessment        1273 non-null int64\n",
      "pathology         1273 non-null object\n",
      "subtlety          1273 non-null int64\n",
      "od_img_path       1273 non-null object\n",
      "od_crop_path      1273 non-null object\n",
      "mask_path         1273 non-null object\n",
      "dtypes: int64(4), object(9)\n",
      "memory usage: 139.2+ KB\n"
     ]
    }
   ],
   "source": [
    "semantics_path = \"mass_case_description_train_set.csv\"\n",
    "semantic_df = pd.read_csv(semantics_path)\n",
    "semantic_df.dropna(inplace=True)\n",
    "semantic_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_feature_names = ['breast_density', 'abn_num', 'mass_shape', 'mass_margins', 'assessment']\n",
    "semantic_label_name = ['pathology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '2' '3' '4']\n",
      "['1' '2' '3' '4' '5' '6']\n",
      "['ARCHITECTURAL_DISTORTION' 'ASYMMETRIC_BREAST_TISSUE'\n",
      " 'FOCAL_ASYMMETRIC_DENSITY' 'IRREGULAR'\n",
      " 'IRREGULAR-ARCHITECTURAL_DISTORTION' 'IRREGULAR-FOCAL_ASYMMETRIC_DENSITY'\n",
      " 'LOBULATED' 'LOBULATED-ARCHITECTURAL_DISTORTION' 'LOBULATED-IRREGULAR'\n",
      " 'LOBULATED-LYMPH_NODE' 'LOBULATED-OVAL' 'LYMPH_NODE' 'OVAL'\n",
      " 'OVAL-LYMPH_NODE' 'ROUND' 'ROUND-IRREGULAR-ARCHITECTURAL_DISTORTION'\n",
      " 'ROUND-LOBULATED' 'ROUND-OVAL']\n",
      "['CIRCUMSCRIBED' 'CIRCUMSCRIBED-ILL_DEFINED'\n",
      " 'CIRCUMSCRIBED-MICROLOBULATED' 'CIRCUMSCRIBED-OBSCURED' 'ILL_DEFINED'\n",
      " 'ILL_DEFINED-SPICULATED' 'MICROLOBULATED' 'MICROLOBULATED-ILL_DEFINED'\n",
      " 'MICROLOBULATED-ILL_DEFINED-SPICULATED' 'MICROLOBULATED-SPICULATED'\n",
      " 'OBSCURED' 'OBSCURED-ILL_DEFINED' 'OBSCURED-ILL_DEFINED-SPICULATED'\n",
      " 'OBSCURED-SPICULATED' 'SPICULATED']\n",
      "['0' '1' '2' '3' '4' '5']\n"
     ]
    }
   ],
   "source": [
    "for feature in semantic_feature_names:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    #print(semantic_df[feature].values)\n",
    "    #print(semantic_df[semantic_df[feature].isnull() == True])\n",
    "    le.fit(list(semantic_df[feature].astype(str)))\n",
    "    print(le.classes_)\n",
    "    semantic_df[feature] = le.transform(semantic_df[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_patientIDs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-d7f427359864>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_patientIDs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'total_patientIDs' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder(sparse=False)\n",
    "semantic_one_hots = enc.fit_transform(semantic_df[semantic_feature_names])\n",
    "has_semantic = [s1 + \"_\" + s2 + \"_\" + s3 for (s1, s2, s3) in zip(list(semantic_df['patient_id']), list(semantic_df['side']), list(semantic_df['view']))]\n",
    "semantic_encoded_df = pd.DataFrame(semantic_one_hots, index=has_semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precomputed_df = pd.read_csv(precomputed_path)\n",
    "# total_patientIDs = precomputed_df.values[:,0]\n",
    "# missing_semantics = []\n",
    "# for id1 in total_patientIDs:\n",
    "#     if id1 not in has_semantic:\n",
    "#         missing_semantics.append(id1)\n",
    "\n",
    "# print(len(missing_semantics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1599\n",
      "1273\n",
      "669\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 669 entries, P_00668_LEFT_MLO to P_00947_RIGHT_MLO\n",
      "Data columns (total 49 columns):\n",
      "0     669 non-null float64\n",
      "1     669 non-null float64\n",
      "2     669 non-null float64\n",
      "3     669 non-null float64\n",
      "4     669 non-null float64\n",
      "5     669 non-null float64\n",
      "6     669 non-null float64\n",
      "7     669 non-null float64\n",
      "8     669 non-null float64\n",
      "9     669 non-null float64\n",
      "10    669 non-null float64\n",
      "11    669 non-null float64\n",
      "12    669 non-null float64\n",
      "13    669 non-null float64\n",
      "14    669 non-null float64\n",
      "15    669 non-null float64\n",
      "16    669 non-null float64\n",
      "17    669 non-null float64\n",
      "18    669 non-null float64\n",
      "19    669 non-null float64\n",
      "20    669 non-null float64\n",
      "21    669 non-null float64\n",
      "22    669 non-null float64\n",
      "23    669 non-null float64\n",
      "24    669 non-null float64\n",
      "25    669 non-null float64\n",
      "26    669 non-null float64\n",
      "27    669 non-null float64\n",
      "28    669 non-null float64\n",
      "29    669 non-null float64\n",
      "30    669 non-null float64\n",
      "31    669 non-null float64\n",
      "32    669 non-null float64\n",
      "33    669 non-null float64\n",
      "34    669 non-null float64\n",
      "35    669 non-null float64\n",
      "36    669 non-null float64\n",
      "37    669 non-null float64\n",
      "38    669 non-null float64\n",
      "39    669 non-null float64\n",
      "40    669 non-null float64\n",
      "41    669 non-null float64\n",
      "42    669 non-null float64\n",
      "43    669 non-null float64\n",
      "44    669 non-null float64\n",
      "45    669 non-null float64\n",
      "46    669 non-null float64\n",
      "47    669 non-null float64\n",
      "48    669 non-null float64\n",
      "dtypes: float64(49)\n",
      "memory usage: 261.3+ KB\n"
     ]
    }
   ],
   "source": [
    "precomputed_df = pd.read_csv(precomputed_path)\n",
    "total_patientIDs = precomputed_df.values[:,0]\n",
    "print(len(total_patientIDs))\n",
    "print(len(has_semantic))\n",
    "missing_semantics = set(total_patientIDs).difference(has_semantic)\n",
    "print(len(missing_semantics))\n",
    "_, one_hot_length = semantic_one_hots.shape\n",
    "missing_semantics_df = pd.DataFrame(np.zeros((len(missing_semantics), one_hot_length)), index=missing_semantics)\n",
    "missing_semantics_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1942 entries, P_00001_LEFT_CC to P_00947_RIGHT_MLO\n",
      "Data columns (total 49 columns):\n",
      "0     1942 non-null float64\n",
      "1     1942 non-null float64\n",
      "2     1942 non-null float64\n",
      "3     1942 non-null float64\n",
      "4     1942 non-null float64\n",
      "5     1942 non-null float64\n",
      "6     1942 non-null float64\n",
      "7     1942 non-null float64\n",
      "8     1942 non-null float64\n",
      "9     1942 non-null float64\n",
      "10    1942 non-null float64\n",
      "11    1942 non-null float64\n",
      "12    1942 non-null float64\n",
      "13    1942 non-null float64\n",
      "14    1942 non-null float64\n",
      "15    1942 non-null float64\n",
      "16    1942 non-null float64\n",
      "17    1942 non-null float64\n",
      "18    1942 non-null float64\n",
      "19    1942 non-null float64\n",
      "20    1942 non-null float64\n",
      "21    1942 non-null float64\n",
      "22    1942 non-null float64\n",
      "23    1942 non-null float64\n",
      "24    1942 non-null float64\n",
      "25    1942 non-null float64\n",
      "26    1942 non-null float64\n",
      "27    1942 non-null float64\n",
      "28    1942 non-null float64\n",
      "29    1942 non-null float64\n",
      "30    1942 non-null float64\n",
      "31    1942 non-null float64\n",
      "32    1942 non-null float64\n",
      "33    1942 non-null float64\n",
      "34    1942 non-null float64\n",
      "35    1942 non-null float64\n",
      "36    1942 non-null float64\n",
      "37    1942 non-null float64\n",
      "38    1942 non-null float64\n",
      "39    1942 non-null float64\n",
      "40    1942 non-null float64\n",
      "41    1942 non-null float64\n",
      "42    1942 non-null float64\n",
      "43    1942 non-null float64\n",
      "44    1942 non-null float64\n",
      "45    1942 non-null float64\n",
      "46    1942 non-null float64\n",
      "47    1942 non-null float64\n",
      "48    1942 non-null float64\n",
      "dtypes: float64(49)\n",
      "memory usage: 758.6+ KB\n"
     ]
    }
   ],
   "source": [
    "semantic_encoded_df = semantic_encoded_df.append(missing_semantics_df)\n",
    "semantic_encoded_df.info()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
